{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 결측치에 강인한 HAR"
      ],
      "metadata": {
        "id": "QIbFl-7e2TpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# 1. UCI-HAR Raw 데이터 로더\n",
        "# ============================================================================\n",
        "class UCIHARRawDataset(Dataset):\n",
        "    \"\"\"\n",
        "    UCI-HAR Raw 데이터셋\n",
        "    - 128 timesteps × 9 channels (3-axis accelerometer + 3-axis gyroscope)\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, split='train'):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split = split\n",
        "\n",
        "        # Load data\n",
        "        if split == 'train':\n",
        "            self.data = self._load_split('train')\n",
        "        else:\n",
        "            self.data = self._load_split('test')\n",
        "\n",
        "        self.signals, self.labels = self.data\n",
        "\n",
        "    def _load_split(self, split):\n",
        "        \"\"\"Load UCI-HAR raw inertial signals\"\"\"\n",
        "        signals_dir = self.data_dir / split / 'Inertial Signals'\n",
        "\n",
        "        # Signal files\n",
        "        signal_types = [\n",
        "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "        ]\n",
        "\n",
        "        signals = []\n",
        "        for sig_type in signal_types:\n",
        "            filename = signals_dir / f'{sig_type}_{split}.txt'\n",
        "            data = np.loadtxt(filename)\n",
        "            signals.append(data)\n",
        "\n",
        "        # Stack: [N, 128, 9]\n",
        "        signals = np.stack(signals, axis=-1)\n",
        "\n",
        "        # Load labels\n",
        "        labels_file = self.data_dir / split / f'y_{split}.txt'\n",
        "        labels = np.loadtxt(labels_file, dtype=np.int32) - 1  # 0-indexed\n",
        "\n",
        "        return signals, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.signals[idx])  # [128, 9]\n",
        "        y = torch.LongTensor([self.labels[idx]])[0]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Augmentation Strategies (데이터/증강 관점 베이스라인)\n",
        "# ============================================================================\n",
        "class TemporalMaskingBatch:\n",
        "    \"\"\"연속 구간 마스킹 (Ours)\"\"\"\n",
        "    def __init__(self, mask_ratio=0.3, mask_length_range=(10, 30)):\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_length_range = mask_length_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, C] tensor\n",
        "        Returns:\n",
        "            masked_x: [B, T, C], mask: [B, T]\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        max_len = min(self.mask_length_range[1], T)\n",
        "        min_len = min(self.mask_length_range[0], T)\n",
        "\n",
        "        for b in range(B):\n",
        "            num_to_mask = int(T * self.mask_ratio)\n",
        "\n",
        "            while num_to_mask > 0:\n",
        "                length = np.random.randint(min_len, max_len + 1)\n",
        "                length = min(length, num_to_mask, T)\n",
        "                start = np.random.randint(0, T - length + 1)\n",
        "                mask[b, start:start + length] = False\n",
        "                num_to_mask -= length\n",
        "\n",
        "        masked_x = x.clone()\n",
        "        masked_x[~mask] = 0.0\n",
        "\n",
        "        return masked_x, mask\n",
        "\n",
        "\n",
        "class RandomPointDrop:\n",
        "    \"\"\"랜덤 포인트 드롭 (비연속 결측)\"\"\"\n",
        "    def __init__(self, drop_ratio=0.3):\n",
        "        self.drop_ratio = drop_ratio\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, C] tensor\n",
        "        Returns:\n",
        "            dropped_x: [B, T, C], mask: [B, T]\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "        mask = torch.rand(B, T, device=x.device) > self.drop_ratio\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        dropped_x[~mask] = 0.0\n",
        "\n",
        "        return dropped_x, mask\n",
        "\n",
        "\n",
        "class ChannelDrop:\n",
        "    \"\"\"채널/센서 드롭 (센서 고장 시뮬레이션)\"\"\"\n",
        "    def __init__(self, drop_prob=0.3, mode='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            drop_prob: probability to drop channels\n",
        "            mode: 'random' (individual channels), 'axis' (3-channel groups),\n",
        "                  'sensor' (acc/gyro groups)\n",
        "        \"\"\"\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, C] tensor (C=9: acc_xyz, gyro_xyz, total_acc_xyz)\n",
        "        Returns:\n",
        "            dropped_x: [B, T, C], mask: [B, C] (channel-wise mask)\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        if self.mode == 'random':\n",
        "            # Individual channel drop\n",
        "            channel_mask = torch.rand(B, C, device=x.device) > self.drop_prob\n",
        "\n",
        "        elif self.mode == 'axis':\n",
        "            # Drop entire axis (3 channels at once: x, y, or z)\n",
        "            # Channels: [acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z, total_x, total_y, total_z]\n",
        "            # Groups: [0,3,6], [1,4,7], [2,5,8]\n",
        "            axis_groups = [[0,3,6], [1,4,7], [2,5,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in axis_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        elif self.mode == 'sensor':\n",
        "            # Drop entire sensor (acc: 0-2, gyro: 3-5, total_acc: 6-8)\n",
        "            sensor_groups = [[0,1,2], [3,4,5], [6,7,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in sensor_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        # Expand mask: [B, C] → [B, T, C]\n",
        "        dropped_x *= channel_mask.unsqueeze(1).float()\n",
        "\n",
        "        # For compatibility, return temporal mask (all True if any channel observed)\n",
        "        temporal_mask = channel_mask.any(dim=1, keepdim=True).expand(B, T)\n",
        "\n",
        "        return dropped_x, temporal_mask\n",
        "\n",
        "\n",
        "class StandardAugmentation:\n",
        "    \"\"\"표준 SSL 증강 (결측 없음 - SimCLR/BYOL 스타일)\"\"\"\n",
        "    def __init__(self, noise_std=0.05, scale_range=(0.8, 1.2)):\n",
        "        self.noise_std = noise_std\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, C] tensor\n",
        "        Returns:\n",
        "            augmented_x: [B, T, C], mask: [B, T] (all True)\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Gaussian noise\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        augmented_x = x + noise\n",
        "\n",
        "        # Random scaling\n",
        "        scale = torch.empty(B, 1, C, device=x.device).uniform_(*self.scale_range)\n",
        "        augmented_x = augmented_x * scale\n",
        "\n",
        "        # No masking (all observed)\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        return augmented_x, mask\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ELK Backbone\n",
        "# ============================================================================\n",
        "class ELKBlock(nn.Module):\n",
        "    \"\"\"Efficient Large Kernel Block with structural reparameterization.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
        "        super().__init__()\n",
        "        self.deploy = deploy\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding_large1 = kernel_size // 2\n",
        "        kernel_size_large2 = kernel_size - 2\n",
        "        padding_large2 = kernel_size_large2 // 2\n",
        "        kernel_size_small1 = 5\n",
        "        padding_small1 = kernel_size_small1 // 2\n",
        "        kernel_size_small2 = 3\n",
        "        padding_small2 = kernel_size_small2 // 2\n",
        "\n",
        "        if deploy:\n",
        "            self.reparam_conv = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=True\n",
        "            )\n",
        "        else:\n",
        "            self.dw_large1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_large2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_large2,\n",
        "                padding=padding_large2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small1,\n",
        "                padding=padding_small1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small2,\n",
        "                padding=padding_small2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
        "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deploy:\n",
        "            x = self.reparam_conv(x)\n",
        "        else:\n",
        "            x1 = self.bn_large1(self.dw_large1(x))\n",
        "            x2 = self.bn_large2(self.dw_large2(x))\n",
        "            x3 = self.bn_small1(self.dw_small1(x))\n",
        "            x4 = self.bn_small2(self.dw_small2(x))\n",
        "            x5 = self.bn_id(x)\n",
        "            x = x1 + x2 + x3 + x4 + x5\n",
        "        x = self.activation(x)\n",
        "        return self.pointwise(x)\n",
        "\n",
        "\n",
        "class ELKBackbone(nn.Module):\n",
        "    \"\"\"ELK Backbone built by stacking ELKBlocks\"\"\"\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=6, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(ELKBlock(d_model, d_model, kernel_size=kernel_size))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.elk_layers = nn.Sequential(*layers)\n",
        "        self.out_channels = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.elk_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Encoder with Masked Pooling\n",
        "# ============================================================================\n",
        "def masked_global_avg_pool(h, mask):\n",
        "    \"\"\"Mask-aware global average pooling\"\"\"\n",
        "    mask_float = mask.unsqueeze(1).float()  # [B, 1, T]\n",
        "    numerator = (h * mask_float).sum(dim=-1)\n",
        "    denominator = mask_float.sum(dim=-1).clamp_min(1e-6)\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "class ELKEncoder(nn.Module):\n",
        "    \"\"\"ELK-based encoder with mask-aware pooling\"\"\"\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=6,\n",
        "                 kernel_size=31, output_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = ELKBackbone(in_channels, d_model, num_layers, kernel_size, dropout)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(d_model, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.transpose(1, 2)  # [B, T, C] → [B, C, T]\n",
        "        h = self.backbone(x)\n",
        "        if mask is not None:\n",
        "            h = masked_global_avg_pool(h, mask)\n",
        "        else:\n",
        "            h = h.mean(dim=-1)\n",
        "        z = self.projection(h)\n",
        "        z = F.normalize(z, dim=1)\n",
        "        return z\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Loss Functions\n",
        "# ============================================================================\n",
        "class MTCLoss(nn.Module):\n",
        "    \"\"\"Masked Temporal Consistency Loss\"\"\"\n",
        "    def forward(self, z_clean, z_masked):\n",
        "        return F.mse_loss(z_clean, z_masked)\n",
        "\n",
        "\n",
        "class SymmetricNTXentLoss(nn.Module):\n",
        "    \"\"\"Symmetric NT-Xent Loss\"\"\"\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        device = z1.device\n",
        "\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        sim_11 = (z1 @ z1.T) / self.temperature\n",
        "        sim_22 = (z2 @ z2.T) / self.temperature\n",
        "        sim_12 = (z1 @ z2.T) / self.temperature\n",
        "        sim_21 = sim_12.T\n",
        "\n",
        "        mask = torch.eye(B, device=device, dtype=torch.bool)\n",
        "        sim_11 = sim_11.masked_fill(mask, -9e15)\n",
        "        sim_22 = sim_22.masked_fill(mask, -9e15)\n",
        "\n",
        "        logits_1 = torch.cat([sim_12, sim_11], dim=1)\n",
        "        logits_2 = torch.cat([sim_21, sim_22], dim=1)\n",
        "        labels = torch.arange(B, device=device)\n",
        "\n",
        "        loss_1 = F.cross_entropy(logits_1, labels)\n",
        "        loss_2 = F.cross_entropy(logits_2, labels)\n",
        "\n",
        "        return 0.5 * (loss_1 + loss_2)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. 통합 프레임워크 (모든 베이스라인 지원)\n",
        "# ============================================================================\n",
        "class UnifiedSSLFramework(nn.Module):\n",
        "    \"\"\"\n",
        "    Unified SSL Framework supporting multiple baselines:\n",
        "    1. SL-Only (Supervised Learning)\n",
        "    2. SSL w/o Missing (Standard Contrastive)\n",
        "    3. Random Point Drop\n",
        "    4. Channel Drop\n",
        "    5. MTC + MICL (Ours)\n",
        "    \"\"\"\n",
        "    def __init__(self, method='mtc_micl', in_channels=9, d_model=128,\n",
        "                 num_layers=6, kernel_size=31, output_dim=256, dropout=0.1,\n",
        "                 mask_ratio=0.3, temperature=0.07,\n",
        "                 lambda_mtc=1.0, lambda_micl=1.0, channel_drop_mode='random'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.method = method\n",
        "        self.encoder = ELKEncoder(in_channels, d_model, num_layers,\n",
        "                                   kernel_size, output_dim, dropout)\n",
        "\n",
        "        # Augmentation strategy\n",
        "        if method == 'sl_only':\n",
        "            self.augmentation = None  # No SSL\n",
        "        elif method == 'ssl_wo_missing':\n",
        "            self.augmentation = StandardAugmentation()\n",
        "        elif method == 'random_point_drop':\n",
        "            self.augmentation = RandomPointDrop(drop_ratio=mask_ratio)\n",
        "        elif method == 'channel_drop':\n",
        "            self.augmentation = ChannelDrop(drop_prob=mask_ratio, mode=channel_drop_mode)\n",
        "        elif method == 'mtc_micl':\n",
        "            self.augmentation = TemporalMaskingBatch(mask_ratio=mask_ratio)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "        # Loss functions\n",
        "        self.mtc_loss = MTCLoss()\n",
        "        self.micl_loss = SymmetricNTXentLoss(temperature=temperature)\n",
        "        self.lambda_mtc = lambda_mtc\n",
        "        self.lambda_micl = lambda_micl\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.method == 'sl_only':\n",
        "            # No SSL pretraining - return dummy loss\n",
        "            return torch.tensor(0.0, device=x.device), {'total': 0.0, 'mtc': 0.0, 'micl': 0.0}\n",
        "\n",
        "        # Clean view\n",
        "        z_clean = self.encoder(x, mask=None)\n",
        "\n",
        "        # Augmented view\n",
        "        x_aug, mask = self.augmentation(x)\n",
        "        z_aug = self.encoder(x_aug, mask=mask)\n",
        "\n",
        "        # Compute losses based on method\n",
        "        if self.method == 'ssl_wo_missing':\n",
        "            # Only contrastive loss (no MTC)\n",
        "            loss_mtc = torch.tensor(0.0, device=x.device)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = loss_micl\n",
        "        else:\n",
        "            # MTC + MICL\n",
        "            loss_mtc = self.mtc_loss(z_clean, z_aug)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = self.lambda_mtc * loss_mtc + self.lambda_micl * loss_micl\n",
        "\n",
        "        losses_dict = {\n",
        "            'total': loss.item(),\n",
        "            'mtc': loss_mtc.item(),\n",
        "            'micl': loss_micl.item()\n",
        "        }\n",
        "\n",
        "        return loss, losses_dict\n",
        "\n",
        "    def get_representation(self, x, mask=None):\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(x, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. Linear Evaluation Protocol\n",
        "# ============================================================================\n",
        "class LinearClassifier(nn.Module):\n",
        "    \"\"\"Linear evaluation protocol\"\"\"\n",
        "    def __init__(self, encoder, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(x, mask=None)\n",
        "        return self.classifier(z)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. Training & Evaluation Functions\n",
        "# ============================================================================\n",
        "def train_ssl_epoch(model, dataloader, optimizer, device):\n",
        "    \"\"\"SSL pretraining epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mtc = 0\n",
        "    total_micl = 0\n",
        "\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, losses_dict = model(x)\n",
        "        if loss.item() > 0:  # Skip SL-only\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total_loss += losses_dict['total']\n",
        "        total_mtc += losses_dict['mtc']\n",
        "        total_micl += losses_dict['micl']\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'mtc': total_mtc / num_batches,\n",
        "        'micl': total_micl / num_batches\n",
        "    }\n",
        "\n",
        "\n",
        "def train_linear_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"Linear evaluation training epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), 100.0 * correct / total\n",
        "\n",
        "\n",
        "def evaluate_linear(model, dataloader, device):\n",
        "    \"\"\"Linear evaluation testing\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "def evaluate_missing_robustness(model, dataloader, device, missing_ratios=[0.0, 0.3, 0.5, 0.7]):\n",
        "    \"\"\"\n",
        "    Evaluate robustness under different missing ratios at test time\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for ratio in missing_ratios:\n",
        "        masking = TemporalMaskingBatch(mask_ratio=ratio)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in dataloader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                if ratio > 0:\n",
        "                    x_masked, mask = masking(x)\n",
        "                    logits = model(x_masked)\n",
        "                else:\n",
        "                    logits = model(x)\n",
        "\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        results[ratio] = 100.0 * correct / total\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. Comprehensive Benchmark\n",
        "# ============================================================================\n",
        "def run_comprehensive_benchmark(data_dir, device, ssl_epochs=50, linear_epochs=50):\n",
        "    \"\"\"\n",
        "    Run comprehensive benchmark comparing all methods\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE BENCHMARK: ELK-MTC-MICL vs Baselines\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Dataset\n",
        "    train_dataset = UCIHARRawDataset(data_dir, split='train')\n",
        "    test_dataset = UCIHARRawDataset(data_dir, split='test')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    methods = [\n",
        "        {'name': 'SL-Only', 'method': 'sl_only', 'desc': 'Supervised learning (no SSL)'},\n",
        "        {'name': 'SSL w/o Missing', 'method': 'ssl_wo_missing', 'desc': 'Standard contrastive (SimCLR-style)'},\n",
        "        {'name': 'Random Point Drop', 'method': 'random_point_drop', 'desc': 'Non-contiguous missing'},\n",
        "        {'name': 'Channel Drop (Random)', 'method': 'channel_drop', 'desc': 'Random channel missing', 'channel_mode': 'random'},\n",
        "        {'name': 'Channel Drop (Sensor)', 'method': 'channel_drop', 'desc': 'Sensor-wise missing', 'channel_mode': 'sensor'},\n",
        "        {'name': 'MTC + MICL (Ours)', 'method': 'mtc_micl', 'desc': 'Contiguous temporal masking + joint loss'},\n",
        "    ]\n",
        "\n",
        "    results = defaultdict(dict)\n",
        "\n",
        "    for config in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Method: {config['name']}\")\n",
        "        print(f\"Description: {config['desc']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # SSL Pretraining\n",
        "        if config['method'] != 'sl_only':\n",
        "            print(f\"\\n[Phase 1] SSL Pretraining ({ssl_epochs} epochs)...\")\n",
        "\n",
        "            ssl_model = UnifiedSSLFramework(\n",
        "                method=config['method'],\n",
        "                channel_drop_mode=config.get('channel_mode', 'random')\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ssl_epochs)\n",
        "\n",
        "            for epoch in range(ssl_epochs):\n",
        "                metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "                scheduler.step()\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/{ssl_epochs}: Loss={metrics['loss']:.4f}, \"\n",
        "                          f\"MTC={metrics['mtc']:.4f}, MICL={metrics['micl']:.4f}\")\n",
        "\n",
        "            encoder = ssl_model.encoder\n",
        "        else:\n",
        "            # SL-Only: random init encoder\n",
        "            encoder = ELKEncoder().to(device)\n",
        "\n",
        "        # Linear Evaluation\n",
        "        print(f\"\\n[Phase 2] Linear Evaluation ({linear_epochs} epochs)...\")\n",
        "        linear_model = LinearClassifier(encoder, num_classes=6).to(device)\n",
        "        optimizer = torch.optim.Adam(linear_model.classifier.parameters(), lr=1e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_test_acc = 0.0\n",
        "        for epoch in range(linear_epochs):\n",
        "            train_loss, train_acc = train_linear_epoch(\n",
        "                linear_model, train_loader, optimizer, criterion, device\n",
        "            )\n",
        "            test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"  Epoch {epoch+1}/{linear_epochs}: Train Acc={train_acc:.2f}%, \"\n",
        "                      f\"Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "        results[config['name']]['test_accuracy'] = best_test_acc\n",
        "\n",
        "        # Missing Robustness Test\n",
        "        print(f\"\\n[Phase 3] Missing Robustness Test...\")\n",
        "        missing_results = evaluate_missing_robustness(\n",
        "            linear_model, test_loader, device,\n",
        "            missing_ratios=[0.0, 0.3, 0.5, 0.7]\n",
        "        )\n",
        "        results[config['name']]['missing_robustness'] = missing_results\n",
        "\n",
        "        for ratio, acc in missing_results.items():\n",
        "            print(f\"  Missing Ratio {ratio:.1f}: {acc:.2f}%\")\n",
        "\n",
        "        # Save model\n",
        "        torch.save({\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'classifier_state_dict': linear_model.classifier.state_dict(),\n",
        "            'config': config,\n",
        "            'results': results[config['name']]\n",
        "        }, f\"benchmark_{config['method']}.pth\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"BENCHMARK SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Method':<25} {'Clean Acc':<12} {'30% Miss':<12} {'50% Miss':<12} {'70% Miss':<12}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for method_name, result in results.items():\n",
        "        clean_acc = result['missing_robustness'][0.0]\n",
        "        miss_30 = result['missing_robustness'][0.3]\n",
        "        miss_50 = result['missing_robustness'][0.5]\n",
        "        miss_70 = result['missing_robustness'][0.7]\n",
        "        print(f\"{method_name:<25} {clean_acc:>10.2f}% {miss_30:>10.2f}% \"\n",
        "              f\"{miss_50:>10.2f}% {miss_70:>10.2f}%\")\n",
        "\n",
        "    # Save results\n",
        "    with open('benchmark_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\n✓ Results saved to: benchmark_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. Main Entry Point\n",
        "# ============================================================================\n",
        "def main():\n",
        "    data_dir = '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\\n\")\n",
        "\n",
        "    # Run comprehensive benchmark\n",
        "    results = run_comprehensive_benchmark(\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        ssl_epochs=50,\n",
        "        linear_epochs=50\n",
        "    )\n",
        "\n",
        "    # Generate comparison plots\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Plot 1: Test Accuracy Comparison\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        methods = list(results.keys())\n",
        "        accuracies = [results[m]['test_accuracy'] for m in methods]\n",
        "        colors = ['red', 'orange', 'yellow', 'lightblue', 'blue', 'green']\n",
        "        bars = plt.bar(range(len(methods)), accuracies, color=colors, alpha=0.7)\n",
        "        plt.xticks(range(len(methods)), methods, rotation=45, ha='right')\n",
        "        plt.ylabel('Test Accuracy (%)')\n",
        "        plt.title('Linear Evaluation: Test Accuracy')\n",
        "        plt.ylim([0, 100])\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        # Plot 2: Missing Robustness\n",
        "        plt.subplot(1, 2, 2)\n",
        "        missing_ratios = [0.0, 0.3, 0.5, 0.7]\n",
        "        for method in methods:\n",
        "            accs = [results[method]['missing_robustness'][r] for r in missing_ratios]\n",
        "            plt.plot(missing_ratios, accs, marker='o', label=method, linewidth=2)\n",
        "\n",
        "        plt.xlabel('Missing Ratio')\n",
        "        plt.ylabel('Test Accuracy (%)')\n",
        "        plt.title('Robustness to Missing Data')\n",
        "        plt.legend(loc='lower left', fontsize=8)\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.ylim([0, 100])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('benchmark_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\n✓ Comparison plots saved to: benchmark_comparison.png\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"\\n⚠ Matplotlib not available. Skipping plots.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. Additional Utility: Single Method Training\n",
        "# ============================================================================\n",
        "def train_single_method(method='mtc_micl', data_dir='./UCI HAR Dataset',\n",
        "                       ssl_epochs=100, linear_epochs=50, device=None):\n",
        "    \"\"\"\n",
        "    Train a single method (for detailed experimentation)\n",
        "\n",
        "    Args:\n",
        "        method: one of ['sl_only', 'ssl_wo_missing', 'random_point_drop',\n",
        "                        'channel_drop', 'mtc_micl']\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training Method: {method}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Dataset\n",
        "    train_dataset = UCIHARRawDataset(data_dir, split='train')\n",
        "    test_dataset = UCIHARRawDataset(data_dir, split='test')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    # SSL Pretraining\n",
        "    if method != 'sl_only':\n",
        "        print(f\"[Phase 1] SSL Pretraining ({ssl_epochs} epochs)...\\n\")\n",
        "\n",
        "        ssl_model = UnifiedSSLFramework(method=method).to(device)\n",
        "        optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ssl_epochs)\n",
        "\n",
        "        for epoch in range(ssl_epochs):\n",
        "            metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "            scheduler.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{ssl_epochs}: Loss={metrics['loss']:.4f}, \"\n",
        "                      f\"MTC={metrics['mtc']:.4f}, MICL={metrics['micl']:.4f}\")\n",
        "\n",
        "        encoder = ssl_model.encoder\n",
        "        torch.save(encoder.state_dict(), f'{method}_encoder.pth')\n",
        "        print(f\"\\n✓ Encoder saved: {method}_encoder.pth\")\n",
        "    else:\n",
        "        encoder = ELKEncoder().to(device)\n",
        "\n",
        "    # Linear Evaluation\n",
        "    print(f\"\\n[Phase 2] Linear Evaluation ({linear_epochs} epochs)...\\n\")\n",
        "    linear_model = LinearClassifier(encoder, num_classes=6).to(device)\n",
        "    optimizer = torch.optim.Adam(linear_model.classifier.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "    for epoch in range(linear_epochs):\n",
        "        train_loss, train_acc = train_linear_epoch(\n",
        "            linear_model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "        test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            torch.save(linear_model.state_dict(), f'{method}_best_classifier.pth')\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{linear_epochs}: Train Acc={train_acc:.2f}%, \"\n",
        "                  f\"Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"\\n✓ Best Test Accuracy: {best_test_acc:.2f}%\")\n",
        "    print(f\"✓ Best model saved: {method}_best_classifier.pth\")\n",
        "\n",
        "    # Missing Robustness\n",
        "    print(f\"\\n[Phase 3] Missing Robustness Test...\\n\")\n",
        "    missing_results = evaluate_missing_robustness(\n",
        "        linear_model, test_loader, device, missing_ratios=[0.0, 0.3, 0.5, 0.7]\n",
        "    )\n",
        "\n",
        "    print(\"Missing Ratio | Test Accuracy\")\n",
        "    print(\"-\" * 30)\n",
        "    for ratio, acc in missing_results.items():\n",
        "        print(f\"    {ratio:.1f}      |    {acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'test_accuracy': best_test_acc,\n",
        "        'missing_robustness': missing_results\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 12. Ablation Study: Loss Components\n",
        "# ============================================================================\n",
        "def run_loss_ablation(data_dir='./UCI HAR Dataset', device=None):\n",
        "    \"\"\"\n",
        "    Ablation study on loss components\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ABLATION STUDY: Loss Components\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    train_dataset = UCIHARRawDataset(data_dir, split='train')\n",
        "    test_dataset = UCIHARRawDataset(data_dir, split='test')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    configs = [\n",
        "        {'name': 'MTC only', 'lambda_mtc': 1.0, 'lambda_micl': 0.0},\n",
        "        {'name': 'MICL only', 'lambda_mtc': 0.0, 'lambda_micl': 1.0},\n",
        "        {'name': 'MTC + MICL (0.5:0.5)', 'lambda_mtc': 0.5, 'lambda_micl': 0.5},\n",
        "        {'name': 'MTC + MICL (1.0:1.0)', 'lambda_mtc': 1.0, 'lambda_micl': 1.0},\n",
        "        {'name': 'MTC + MICL (1.0:0.5)', 'lambda_mtc': 1.0, 'lambda_micl': 0.5},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for config in configs:\n",
        "        print(f\"\\nTraining: {config['name']}\")\n",
        "        print(f\"λ_MTC={config['lambda_mtc']}, λ_MICL={config['lambda_micl']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # SSL\n",
        "        ssl_model = UnifiedSSLFramework(\n",
        "            method='mtc_micl',\n",
        "            lambda_mtc=config['lambda_mtc'],\n",
        "            lambda_micl=config['lambda_micl']\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "        for epoch in range(50):\n",
        "            metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"  Epoch {epoch+1}: Loss={metrics['loss']:.4f}\")\n",
        "\n",
        "        # Linear eval\n",
        "        linear_model = LinearClassifier(ssl_model.encoder, num_classes=6).to(device)\n",
        "        optimizer = torch.optim.Adam(linear_model.classifier.parameters(), lr=1e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_acc = 0.0\n",
        "        for epoch in range(50):\n",
        "            _, _ = train_linear_epoch(linear_model, train_loader, optimizer, criterion, device)\n",
        "            test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "            best_acc = max(best_acc, test_acc)\n",
        "\n",
        "        results[config['name']] = best_acc\n",
        "        print(f\"  → Best Test Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Ablation Study Summary\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for name, acc in results.items():\n",
        "        print(f\"{name:<30} {acc:>10.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. Main Entry Point\n",
        "# ============================================================================\n",
        "def main():\n",
        "    data_dir = '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\\n\")\n",
        "\n",
        "    # Run comprehensive benchmark\n",
        "    results = run_comprehensive_benchmark(\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        ssl_epochs=50,\n",
        "        linear_epochs=50\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA0OuEMn2SxK",
        "outputId": "3388251e-6dd1-4570-835f-325728087e3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "PyTorch version: 2.8.0+cu126\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE BENCHMARK: ELK-MTC-MICL vs Baselines\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Method: SL-Only\n",
            "Description: Supervised learning (no SSL)\n",
            "================================================================================\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=74.96%, Test Acc=58.30%\n",
            "  Epoch 20/50: Train Acc=77.04%, Test Acc=61.79%\n",
            "  Epoch 30/50: Train Acc=78.20%, Test Acc=65.56%\n",
            "  Epoch 40/50: Train Acc=79.03%, Test Acc=65.52%\n",
            "  Epoch 50/50: Train Acc=79.62%, Test Acc=69.63%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 69.63%\n",
            "  Missing Ratio 0.3: 63.42%\n",
            "  Missing Ratio 0.5: 54.94%\n",
            "  Missing Ratio 0.7: 42.93%\n",
            "\n",
            "================================================================================\n",
            "Method: SSL w/o Missing\n",
            "Description: Standard contrastive (SimCLR-style)\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (50 epochs)...\n",
            "  Epoch 10/50: Loss=0.1688, MTC=0.0000, MICL=0.1688\n",
            "  Epoch 20/50: Loss=0.1055, MTC=0.0000, MICL=0.1055\n",
            "  Epoch 30/50: Loss=0.0817, MTC=0.0000, MICL=0.0817\n",
            "  Epoch 40/50: Loss=0.0634, MTC=0.0000, MICL=0.0634\n",
            "  Epoch 50/50: Loss=0.0619, MTC=0.0000, MICL=0.0619\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=59.49%, Test Acc=59.52%\n",
            "  Epoch 20/50: Train Acc=65.14%, Test Acc=63.83%\n",
            "  Epoch 30/50: Train Acc=67.46%, Test Acc=68.00%\n",
            "  Epoch 40/50: Train Acc=70.55%, Test Acc=68.31%\n",
            "  Epoch 50/50: Train Acc=71.75%, Test Acc=71.73%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 71.73%\n",
            "  Missing Ratio 0.3: 48.59%\n",
            "  Missing Ratio 0.5: 41.77%\n",
            "  Missing Ratio 0.7: 35.05%\n",
            "\n",
            "================================================================================\n",
            "Method: Random Point Drop\n",
            "Description: Non-contiguous missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (50 epochs)...\n",
            "  Epoch 10/50: Loss=0.0956, MTC=0.0008, MICL=0.0947\n",
            "  Epoch 20/50: Loss=0.0348, MTC=0.0009, MICL=0.0339\n",
            "  Epoch 30/50: Loss=0.0234, MTC=0.0009, MICL=0.0225\n",
            "  Epoch 40/50: Loss=0.0173, MTC=0.0009, MICL=0.0165\n",
            "  Epoch 50/50: Loss=0.0144, MTC=0.0009, MICL=0.0135\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=49.16%, Test Acc=47.30%\n",
            "  Epoch 20/50: Train Acc=51.82%, Test Acc=49.10%\n",
            "  Epoch 30/50: Train Acc=53.70%, Test Acc=50.15%\n",
            "  Epoch 40/50: Train Acc=55.88%, Test Acc=52.19%\n",
            "  Epoch 50/50: Train Acc=56.37%, Test Acc=54.05%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 54.05%\n",
            "  Missing Ratio 0.3: 49.37%\n",
            "  Missing Ratio 0.5: 43.94%\n",
            "  Missing Ratio 0.7: 39.43%\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Random)\n",
            "Description: Random channel missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (50 epochs)...\n",
            "  Epoch 10/50: Loss=0.6210, MTC=0.0012, MICL=0.6198\n",
            "  Epoch 20/50: Loss=0.2821, MTC=0.0012, MICL=0.2809\n",
            "  Epoch 30/50: Loss=0.1644, MTC=0.0012, MICL=0.1632\n",
            "  Epoch 40/50: Loss=0.1330, MTC=0.0012, MICL=0.1318\n",
            "  Epoch 50/50: Loss=0.1212, MTC=0.0011, MICL=0.1200\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=83.61%, Test Acc=81.91%\n",
            "  Epoch 20/50: Train Acc=86.43%, Test Acc=85.04%\n",
            "  Epoch 30/50: Train Acc=87.24%, Test Acc=87.07%\n",
            "  Epoch 40/50: Train Acc=87.98%, Test Acc=87.55%\n",
            "  Epoch 50/50: Train Acc=88.74%, Test Acc=87.92%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 87.92%\n",
            "  Missing Ratio 0.3: 64.68%\n",
            "  Missing Ratio 0.5: 48.42%\n",
            "  Missing Ratio 0.7: 41.87%\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Sensor)\n",
            "Description: Sensor-wise missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (50 epochs)...\n",
            "  Epoch 10/50: Loss=1.1473, MTC=0.0011, MICL=1.1462\n",
            "  Epoch 20/50: Loss=0.5395, MTC=0.0013, MICL=0.5382\n",
            "  Epoch 30/50: Loss=0.3422, MTC=0.0012, MICL=0.3411\n",
            "  Epoch 40/50: Loss=0.2879, MTC=0.0011, MICL=0.2867\n",
            "  Epoch 50/50: Loss=0.2846, MTC=0.0011, MICL=0.2835\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=77.46%, Test Acc=71.77%\n",
            "  Epoch 20/50: Train Acc=79.54%, Test Acc=74.69%\n",
            "  Epoch 30/50: Train Acc=81.18%, Test Acc=76.82%\n",
            "  Epoch 40/50: Train Acc=82.22%, Test Acc=77.77%\n",
            "  Epoch 50/50: Train Acc=82.49%, Test Acc=78.45%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 78.45%\n",
            "  Missing Ratio 0.3: 51.27%\n",
            "  Missing Ratio 0.5: 44.42%\n",
            "  Missing Ratio 0.7: 39.74%\n",
            "\n",
            "================================================================================\n",
            "Method: MTC + MICL (Ours)\n",
            "Description: Contiguous temporal masking + joint loss\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (50 epochs)...\n",
            "  Epoch 10/50: Loss=0.1498, MTC=0.0008, MICL=0.1489\n",
            "  Epoch 20/50: Loss=0.0918, MTC=0.0009, MICL=0.0909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 30/50: Loss=0.0701, MTC=0.0009, MICL=0.0692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 40/50: Loss=0.0568, MTC=0.0009, MICL=0.0559\n",
            "  Epoch 50/50: Loss=0.0480, MTC=0.0009, MICL=0.0471\n",
            "\n",
            "[Phase 2] Linear Evaluation (50 epochs)...\n",
            "  Epoch 10/50: Train Acc=54.35%, Test Acc=45.30%\n",
            "  Epoch 20/50: Train Acc=58.38%, Test Acc=48.52%\n",
            "  Epoch 30/50: Train Acc=60.95%, Test Acc=52.02%\n",
            "  Epoch 40/50: Train Acc=63.32%, Test Acc=53.38%\n",
            "  Epoch 50/50: Train Acc=64.00%, Test Acc=54.02%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.0: 54.02%\n",
            "  Missing Ratio 0.3: 53.85%\n",
            "  Missing Ratio 0.5: 49.75%\n",
            "  Missing Ratio 0.7: 48.63%\n",
            "\n",
            "================================================================================\n",
            "BENCHMARK SUMMARY\n",
            "================================================================================\n",
            "Method                    Clean Acc    30% Miss     50% Miss     70% Miss    \n",
            "--------------------------------------------------------------------------------\n",
            "SL-Only                        69.63%      63.42%      54.94%      42.93%\n",
            "SSL w/o Missing                71.73%      48.59%      41.77%      35.05%\n",
            "Random Point Drop              54.05%      49.37%      43.94%      39.43%\n",
            "Channel Drop (Random)          87.92%      64.68%      48.42%      41.87%\n",
            "Channel Drop (Sensor)          78.45%      51.27%      44.42%      39.74%\n",
            "MTC + MICL (Ours)              54.02%      53.85%      49.75%      48.63%\n",
            "\n",
            "✓ Results saved to: benchmark_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Missing + Noise-Only 에 강인한 HAR\n",
        "\n",
        "---\n",
        "\n",
        "- Group A: Missing-Only (4개)\n",
        "\n",
        "✓ temporal_30       # 연속 구간 30% 결측\n",
        "\n",
        "✓ temporal_50       # 연속 구간 50% 결측\n",
        "\n",
        "✓ temporal_70       # 연속 구간 70% 결측\n",
        "\n",
        "✓ point_30          # 비연속 포인트 30% 결측\n",
        "\n",
        "- Group B: Noise-Only (4개)\n",
        "\n",
        "✓ gaussian_20db     # 가우시안 노이즈 (낮음, SNR=20dB)\n",
        "\n",
        "✓ gaussian_10db     # 가우시안 노이즈 (중간, SNR=10dB)\n",
        "\n",
        "✓ gaussian_5db      # 가우시안 노이즈 (높음, SNR=5dB)\n",
        "\n",
        "✓ salt_pepper_10    # 충격 잡음 10%\n",
        "\n",
        "- Group C: Combined (4개)\n",
        "\n",
        "✓ temporal30_gaussian10      # 결측 30% + 가우시안 10dB\n",
        "\n",
        "✓ temporal50_gaussian10      # 결측 50% + 가우시안 10dB\n",
        "\n",
        "✓ temporal30_saltpepper      # 결측 30% + 충격 잡음 10%\n",
        "\n",
        "✓ temporal50_saltpepper      # 결측 50% + 충격 잡음 10%"
      ],
      "metadata": {
        "id": "XcKfE-Jv_0WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============================================================================\n",
        "# 1. UCI-HAR Raw 데이터 로더\n",
        "# ============================================================================\n",
        "class UCIHARRawDataset(Dataset):\n",
        "    \"\"\"UCI-HAR Raw 데이터셋\"\"\"\n",
        "    def __init__(self, data_dir, split='train'):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            self.data = self._load_split('train')\n",
        "        else:\n",
        "            self.data = self._load_split('test')\n",
        "\n",
        "        self.signals, self.labels = self.data\n",
        "\n",
        "    def _load_split(self, split):\n",
        "        signals_dir = self.data_dir / split / 'Inertial Signals'\n",
        "\n",
        "        signal_types = [\n",
        "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "        ]\n",
        "\n",
        "        signals = []\n",
        "        for sig_type in signal_types:\n",
        "            filename = signals_dir / f'{sig_type}_{split}.txt'\n",
        "            data = np.loadtxt(filename)\n",
        "            signals.append(data)\n",
        "\n",
        "        signals = np.stack(signals, axis=-1)\n",
        "\n",
        "        labels_file = self.data_dir / split / f'y_{split}.txt'\n",
        "        labels = np.loadtxt(labels_file, dtype=np.int32) - 1\n",
        "\n",
        "        return signals, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.signals[idx])\n",
        "        y = torch.LongTensor([self.labels[idx]])[0]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Noise Generators\n",
        "# ============================================================================\n",
        "class GaussianNoise:\n",
        "    \"\"\"백색 가우시안 잡음\"\"\"\n",
        "    def __init__(self, snr_db=10):\n",
        "        self.snr_db = snr_db\n",
        "\n",
        "    def __call__(self, x):\n",
        "        signal_power = torch.mean(x ** 2)\n",
        "        snr_linear = 10 ** (self.snr_db / 10)\n",
        "        noise_power = signal_power / snr_linear\n",
        "        noise = torch.randn_like(x) * torch.sqrt(noise_power)\n",
        "        return x + noise\n",
        "\n",
        "\n",
        "class SaltPepperNoise:\n",
        "    \"\"\"충격 잡음\"\"\"\n",
        "    def __init__(self, corruption_ratio=0.1):\n",
        "        self.corruption_ratio = corruption_ratio\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        mask = torch.rand(B, T, C, device=x.device) < self.corruption_ratio\n",
        "        x_min, x_max = x.min(), x.max()\n",
        "        salt_or_pepper = torch.rand(B, T, C, device=x.device) > 0.5\n",
        "\n",
        "        corrupted = x.clone()\n",
        "        corrupted[mask & salt_or_pepper] = x_max * 2\n",
        "        corrupted[mask & ~salt_or_pepper] = x_min * 2\n",
        "\n",
        "        return corrupted\n",
        "\n",
        "\n",
        "class DriftNoise:\n",
        "    \"\"\"센서 드리프트\"\"\"\n",
        "    def __init__(self, drift_rate=0.05, mode='linear'):\n",
        "        self.drift_rate = drift_rate\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        t = torch.linspace(0, 1, T, device=x.device).view(1, T, 1)\n",
        "\n",
        "        if self.mode == 'linear':\n",
        "            drift = self.drift_rate * t * torch.randn(B, 1, C, device=x.device)\n",
        "        elif self.mode == 'exponential':\n",
        "            drift = self.drift_rate * (torch.exp(t) - 1) * torch.randn(B, 1, C, device=x.device)\n",
        "\n",
        "        return x + drift\n",
        "\n",
        "\n",
        "class BurstNoise:\n",
        "    \"\"\"간헐적 강한 잡음\"\"\"\n",
        "    def __init__(self, burst_ratio=0.15, burst_length_range=(5, 15), intensity=5.0):\n",
        "        self.burst_ratio = burst_ratio\n",
        "        self.burst_length_range = burst_length_range\n",
        "        self.intensity = intensity\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        noisy_x = x.clone()\n",
        "\n",
        "        for b in range(B):\n",
        "            num_to_burst = int(T * self.burst_ratio)\n",
        "\n",
        "            while num_to_burst > 0:\n",
        "                length = np.random.randint(*self.burst_length_range)\n",
        "                length = min(length, num_to_burst, T)\n",
        "                start = np.random.randint(0, T - length + 1)\n",
        "\n",
        "                burst = torch.randn(length, C, device=x.device) * self.intensity * x[b].std()\n",
        "                noisy_x[b, start:start+length] += burst\n",
        "\n",
        "                num_to_burst -= length\n",
        "\n",
        "        return noisy_x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Missing Pattern Generators\n",
        "# ============================================================================\n",
        "class TemporalMaskingBatch:\n",
        "    \"\"\"연속 구간 마스킹\"\"\"\n",
        "    def __init__(self, mask_ratio=0.3, mask_length_range=(10, 30)):\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_length_range = mask_length_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        max_len = min(self.mask_length_range[1], T)\n",
        "        min_len = min(self.mask_length_range[0], T)\n",
        "\n",
        "        for b in range(B):\n",
        "            num_to_mask = int(T * self.mask_ratio)\n",
        "\n",
        "            while num_to_mask > 0:\n",
        "                length = np.random.randint(min_len, max_len + 1)\n",
        "                length = min(length, num_to_mask, T)\n",
        "                start = np.random.randint(0, T - length + 1)\n",
        "                mask[b, start:start + length] = False\n",
        "                num_to_mask -= length\n",
        "\n",
        "        masked_x = x.clone()\n",
        "        masked_x[~mask] = 0.0\n",
        "\n",
        "        return masked_x, mask\n",
        "\n",
        "\n",
        "class RandomPointDrop:\n",
        "    \"\"\"비연속 포인트 드롭\"\"\"\n",
        "    def __init__(self, drop_ratio=0.3):\n",
        "        self.drop_ratio = drop_ratio\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        mask = torch.rand(B, T, device=x.device) > self.drop_ratio\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        dropped_x[~mask] = 0.0\n",
        "\n",
        "        return dropped_x, mask\n",
        "\n",
        "\n",
        "class ChannelDrop:\n",
        "    \"\"\"채널/센서 드롭\"\"\"\n",
        "    def __init__(self, drop_prob=0.3, mode='random'):\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        if self.mode == 'random':\n",
        "            channel_mask = torch.rand(B, C, device=x.device) > self.drop_prob\n",
        "\n",
        "        elif self.mode == 'axis':\n",
        "            axis_groups = [[0,3,6], [1,4,7], [2,5,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in axis_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        elif self.mode == 'sensor':\n",
        "            sensor_groups = [[0,1,2], [3,4,5], [6,7,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in sensor_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        dropped_x *= channel_mask.unsqueeze(1).float()\n",
        "\n",
        "        temporal_mask = channel_mask.any(dim=1, keepdim=True).expand(B, T)\n",
        "\n",
        "        return dropped_x, temporal_mask\n",
        "\n",
        "\n",
        "class StandardAugmentation:\n",
        "    \"\"\"표준 SSL 증강\"\"\"\n",
        "    def __init__(self, noise_std=0.05, scale_range=(0.8, 1.2)):\n",
        "        self.noise_std = noise_std\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        augmented_x = x + noise\n",
        "\n",
        "        scale = torch.empty(B, 1, C, device=x.device).uniform_(*self.scale_range)\n",
        "        augmented_x = augmented_x * scale\n",
        "\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        return augmented_x, mask\n",
        "\n",
        "\n",
        "class HybridCorruption:\n",
        "    \"\"\"결측 + 노이즈 동시 적용\"\"\"\n",
        "    def __init__(self, missing_aug, noise_aug):\n",
        "        self.missing_aug = missing_aug\n",
        "        self.noise_aug = noise_aug\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x_noisy = self.noise_aug(x)\n",
        "        x_corrupted, mask = self.missing_aug(x_noisy)\n",
        "        return x_corrupted, mask\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ELK Backbone\n",
        "# ============================================================================\n",
        "class ELKBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
        "        super().__init__()\n",
        "        self.deploy = deploy\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding_large1 = kernel_size // 2\n",
        "        kernel_size_large2 = kernel_size - 2\n",
        "        padding_large2 = kernel_size_large2 // 2\n",
        "        kernel_size_small1 = 5\n",
        "        padding_small1 = kernel_size_small1 // 2\n",
        "        kernel_size_small2 = 3\n",
        "        padding_small2 = kernel_size_small2 // 2\n",
        "\n",
        "        if deploy:\n",
        "            self.reparam_conv = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=True\n",
        "            )\n",
        "        else:\n",
        "            self.dw_large1 = nn.Conv1d(in_channels, in_channels, kernel_size,\n",
        "                                       padding=padding_large1, groups=in_channels, bias=False)\n",
        "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_large2 = nn.Conv1d(in_channels, in_channels, kernel_size_large2,\n",
        "                                       padding=padding_large2, groups=in_channels, bias=False)\n",
        "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small1 = nn.Conv1d(in_channels, in_channels, kernel_size_small1,\n",
        "                                       padding=padding_small1, groups=in_channels, bias=False)\n",
        "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small2 = nn.Conv1d(in_channels, in_channels, kernel_size_small2,\n",
        "                                       padding=padding_small2, groups=in_channels, bias=False)\n",
        "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
        "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deploy:\n",
        "            x = self.reparam_conv(x)\n",
        "        else:\n",
        "            x1 = self.bn_large1(self.dw_large1(x))\n",
        "            x2 = self.bn_large2(self.dw_large2(x))\n",
        "            x3 = self.bn_small1(self.dw_small1(x))\n",
        "            x4 = self.bn_small2(self.dw_small2(x))\n",
        "            x5 = self.bn_id(x)\n",
        "            x = x1 + x2 + x3 + x4 + x5\n",
        "        x = self.activation(x)\n",
        "        return self.pointwise(x)\n",
        "\n",
        "\n",
        "class ELKBackbone(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=6, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        layers = []\n",
        "        for _ in range(num_layers):\n",
        "            layers.append(ELKBlock(d_model, d_model, kernel_size=kernel_size))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.elk_layers = nn.Sequential(*layers)\n",
        "        self.out_channels = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.elk_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def masked_global_avg_pool(h, mask):\n",
        "    mask_float = mask.unsqueeze(1).float()\n",
        "    numerator = (h * mask_float).sum(dim=-1)\n",
        "    denominator = mask_float.sum(dim=-1).clamp_min(1e-6)\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "class ELKEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=6,\n",
        "                 kernel_size=31, output_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = ELKBackbone(in_channels, d_model, num_layers, kernel_size, dropout)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(d_model, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = self.backbone(x)\n",
        "        if mask is not None:\n",
        "            h = masked_global_avg_pool(h, mask)\n",
        "        else:\n",
        "            h = h.mean(dim=-1)\n",
        "        z = self.projection(h)\n",
        "        z = F.normalize(z, dim=1)\n",
        "        return z\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. Loss Functions\n",
        "# ============================================================================\n",
        "class MTCLoss(nn.Module):\n",
        "    def forward(self, z_clean, z_masked):\n",
        "        return F.mse_loss(z_clean, z_masked)\n",
        "\n",
        "\n",
        "class SymmetricNTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        device = z1.device\n",
        "\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        sim_11 = (z1 @ z1.T) / self.temperature\n",
        "        sim_22 = (z2 @ z2.T) / self.temperature\n",
        "        sim_12 = (z1 @ z2.T) / self.temperature\n",
        "        sim_21 = sim_12.T\n",
        "\n",
        "        mask = torch.eye(B, device=device, dtype=torch.bool)\n",
        "        sim_11 = sim_11.masked_fill(mask, -9e15)\n",
        "        sim_22 = sim_22.masked_fill(mask, -9e15)\n",
        "\n",
        "        logits_1 = torch.cat([sim_12, sim_11], dim=1)\n",
        "        logits_2 = torch.cat([sim_21, sim_22], dim=1)\n",
        "        labels = torch.arange(B, device=device)\n",
        "\n",
        "        loss_1 = F.cross_entropy(logits_1, labels)\n",
        "        loss_2 = F.cross_entropy(logits_2, labels)\n",
        "\n",
        "        return 0.5 * (loss_1 + loss_2)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. Unified SSL Framework\n",
        "# ============================================================================\n",
        "class UnifiedSSLFramework(nn.Module):\n",
        "    def __init__(self, method='mtc_micl', in_channels=9, d_model=128,\n",
        "                 num_layers=6, kernel_size=31, output_dim=256, dropout=0.1,\n",
        "                 mask_ratio=0.3, temperature=0.07,\n",
        "                 lambda_mtc=1.0, lambda_micl=1.0, channel_drop_mode='random'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.method = method\n",
        "        self.encoder = ELKEncoder(in_channels, d_model, num_layers,\n",
        "                                   kernel_size, output_dim, dropout)\n",
        "\n",
        "        if method == 'sl_only':\n",
        "            self.augmentation = None\n",
        "        elif method == 'ssl_wo_missing':\n",
        "            self.augmentation = StandardAugmentation()\n",
        "        elif method == 'random_point_drop':\n",
        "            self.augmentation = RandomPointDrop(drop_ratio=mask_ratio)\n",
        "        elif method == 'channel_drop':\n",
        "            self.augmentation = ChannelDrop(drop_prob=mask_ratio, mode=channel_drop_mode)\n",
        "        elif method == 'mtc_micl':\n",
        "            self.augmentation = TemporalMaskingBatch(mask_ratio=mask_ratio)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "        self.mtc_loss = MTCLoss()\n",
        "        self.micl_loss = SymmetricNTXentLoss(temperature=temperature)\n",
        "        self.lambda_mtc = lambda_mtc\n",
        "        self.lambda_micl = lambda_micl\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.method == 'sl_only':\n",
        "            return torch.tensor(0.0, device=x.device), {'total': 0.0, 'mtc': 0.0, 'micl': 0.0}\n",
        "\n",
        "        z_clean = self.encoder(x, mask=None)\n",
        "        x_aug, mask = self.augmentation(x)\n",
        "        z_aug = self.encoder(x_aug, mask=mask)\n",
        "\n",
        "        if self.method == 'ssl_wo_missing':\n",
        "            loss_mtc = torch.tensor(0.0, device=x.device)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = loss_micl\n",
        "        else:\n",
        "            loss_mtc = self.mtc_loss(z_clean, z_aug)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = self.lambda_mtc * loss_mtc + self.lambda_micl * loss_micl\n",
        "\n",
        "        losses_dict = {\n",
        "            'total': loss.item(),\n",
        "            'mtc': loss_mtc.item(),\n",
        "            'micl': loss_micl.item()\n",
        "        }\n",
        "\n",
        "        return loss, losses_dict\n",
        "\n",
        "    def get_representation(self, x, mask=None):\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(x, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. Linear Classifier\n",
        "# ============================================================================\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(x, mask=None)\n",
        "        return self.classifier(z)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. Robustness Scenario Creation\n",
        "# ============================================================================\n",
        "def create_robustness_scenarios():\n",
        "    \"\"\"12가지 핵심 robustness 시나리오\"\"\"\n",
        "    scenarios = {\n",
        "        # Group A: Missing-Only\n",
        "        'temporal_30': {\n",
        "            'corruption': TemporalMaskingBatch(mask_ratio=0.3),\n",
        "            'description': 'Temporal missing 30%',\n",
        "            'group': 'Missing-Only'\n",
        "        },\n",
        "        'temporal_50': {\n",
        "            'corruption': TemporalMaskingBatch(mask_ratio=0.5),\n",
        "            'description': 'Temporal missing 50%',\n",
        "            'group': 'Missing-Only'\n",
        "        },\n",
        "        'temporal_70': {\n",
        "            'corruption': TemporalMaskingBatch(mask_ratio=0.7),\n",
        "            'description': 'Temporal missing 70%',\n",
        "            'group': 'Missing-Only'\n",
        "        },\n",
        "        'point_30': {\n",
        "            'corruption': RandomPointDrop(drop_ratio=0.3),\n",
        "            'description': 'Point missing 30%',\n",
        "            'group': 'Missing-Only'\n",
        "        },\n",
        "\n",
        "        # Group B: Noise-Only\n",
        "        'gaussian_20db': {\n",
        "            'corruption': GaussianNoise(snr_db=20),\n",
        "            'description': 'Gaussian noise (SNR=20dB)',\n",
        "            'group': 'Noise-Only'\n",
        "        },\n",
        "        'gaussian_10db': {\n",
        "            'corruption': GaussianNoise(snr_db=10),\n",
        "            'description': 'Gaussian noise (SNR=10dB)',\n",
        "            'group': 'Noise-Only'\n",
        "        },\n",
        "        'gaussian_5db': {\n",
        "            'corruption': GaussianNoise(snr_db=5),\n",
        "            'description': 'Gaussian noise (SNR=5dB)',\n",
        "            'group': 'Noise-Only'\n",
        "        },\n",
        "        'salt_pepper_10': {\n",
        "            'corruption': SaltPepperNoise(corruption_ratio=0.1),\n",
        "            'description': 'Salt-pepper noise 10%',\n",
        "            'group': 'Noise-Only'\n",
        "        },\n",
        "\n",
        "        # Group C: Combined\n",
        "        'temporal30_gaussian10': {\n",
        "            'corruption': HybridCorruption(\n",
        "                TemporalMaskingBatch(0.3),\n",
        "                GaussianNoise(10)\n",
        "            ),\n",
        "            'description': 'Temporal 30% + Gaussian 10dB',\n",
        "            'group': 'Combined'\n",
        "        },\n",
        "        'temporal50_gaussian10': {\n",
        "            'corruption': HybridCorruption(\n",
        "                TemporalMaskingBatch(0.5),\n",
        "                GaussianNoise(10)\n",
        "            ),\n",
        "            'description': 'Temporal 50% + Gaussian 10dB',\n",
        "            'group': 'Combined'\n",
        "        },\n",
        "        'temporal30_saltpepper': {\n",
        "            'corruption': HybridCorruption(\n",
        "                TemporalMaskingBatch(0.3),\n",
        "                SaltPepperNoise(0.1)\n",
        "            ),\n",
        "            'description': 'Temporal 30% + Salt-pepper 10%',\n",
        "            'group': 'Combined'\n",
        "        },\n",
        "        'temporal50_saltpepper': {\n",
        "            'corruption': HybridCorruption(\n",
        "                TemporalMaskingBatch(0.5),\n",
        "                SaltPepperNoise(0.1)\n",
        "            ),\n",
        "            'description': 'Temporal 50% + Salt-pepper 10%',\n",
        "            'group': 'Combined'\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. Training Functions\n",
        "# ============================================================================\n",
        "def train_ssl_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mtc = 0\n",
        "    total_micl = 0\n",
        "\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, losses_dict = model(x)\n",
        "        if loss.item() > 0:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total_loss += losses_dict['total']\n",
        "        total_mtc += losses_dict['mtc']\n",
        "        total_micl += losses_dict['micl']\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'mtc': total_mtc / num_batches,\n",
        "        'micl': total_micl / num_batches\n",
        "    }\n",
        "\n",
        "\n",
        "def train_linear_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), 100.0 * correct / total\n",
        "\n",
        "\n",
        "def evaluate_linear(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. Comprehensive Robustness Evaluation\n",
        "# ============================================================================\n",
        "def evaluate_comprehensive_robustness(model, test_loader, device, scenarios):\n",
        "    \"\"\"모든 시나리오에서 robustness 평가\"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPREHENSIVE ROBUSTNESS EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for scenario_name, scenario_config in scenarios.items():\n",
        "        corruption = scenario_config['corruption']\n",
        "        description = scenario_config['description']\n",
        "        group = scenario_config['group']\n",
        "\n",
        "        print(f\"\\n[{group}] {scenario_name}\")\n",
        "        print(f\"  Description: {description}\")\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                # Apply corruption\n",
        "                if isinstance(corruption, HybridCorruption):\n",
        "                    x_corrupted, _ = corruption(x)\n",
        "                elif hasattr(corruption, '__call__'):\n",
        "                    if 'Noise' in corruption.__class__.__name__:\n",
        "                        x_corrupted = corruption(x)\n",
        "                    else:\n",
        "                        x_corrupted, _ = corruption(x)\n",
        "                else:\n",
        "                    x_corrupted = x\n",
        "\n",
        "                logits = model(x_corrupted)\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        accuracy = 100.0 * correct / total\n",
        "        results[scenario_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'group': group,\n",
        "            'description': description\n",
        "        }\n",
        "\n",
        "        print(f\"  Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. Visualization\n",
        "# ============================================================================\n",
        "def plot_robustness_heatmap(all_results, save_path='robustness_heatmap.png'):\n",
        "    \"\"\"Robustness heatmap 생성\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        methods = list(all_results.keys())\n",
        "        scenarios = list(next(iter(all_results.values())).keys())\n",
        "\n",
        "        matrix = []\n",
        "        for method in methods:\n",
        "            row = [all_results[method][scenario]['accuracy'] for scenario in scenarios]\n",
        "            matrix.append(row)\n",
        "\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        sns.heatmap(\n",
        "            matrix,\n",
        "            annot=True,\n",
        "            fmt='.1f',\n",
        "            cmap='RdYlGn',\n",
        "            xticklabels=scenarios,\n",
        "            yticklabels=methods,\n",
        "            vmin=0,\n",
        "            vmax=100,\n",
        "            cbar_kws={'label': 'Accuracy (%)'}\n",
        "        )\n",
        "        plt.title('Robustness Comparison Across Corruption Scenarios',\n",
        "                  fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Corruption Scenario', fontsize=12)\n",
        "        plt.ylabel('Method', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\n✓ Heatmap saved: {save_path}\")\n",
        "    except ImportError:\n",
        "        print(\"\\n⚠ matplotlib/seaborn not available, skipping heatmap generation\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 12. Comprehensive Benchmark\n",
        "# ============================================================================\n",
        "def run_comprehensive_benchmark(data_dir, device, ssl_epochs=50, linear_epochs=50):\n",
        "    \"\"\"전체 벤치마크 실행\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE BENCHMARK: ELK-MTC-MICL vs Baselines\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    train_dataset = UCIHARRawDataset(data_dir, split='train')\n",
        "    test_dataset = UCIHARRawDataset(data_dir, split='test')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    methods = [\n",
        "        {'name': 'SL-Only', 'method': 'sl_only'},\n",
        "        {'name': 'SSL w/o Missing', 'method': 'ssl_wo_missing'},\n",
        "        {'name': 'Random Point Drop', 'method': 'random_point_drop'},\n",
        "        {'name': 'Channel Drop', 'method': 'channel_drop', 'channel_mode': 'random'},\n",
        "        {'name': 'MTC + MICL (Ours)', 'method': 'mtc_micl'},\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "    scenarios = create_robustness_scenarios()\n",
        "\n",
        "    for config in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Method: {config['name']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # SSL Pretraining\n",
        "        if config['method'] != 'sl_only':\n",
        "            print(f\"\\n[Phase 1] SSL Pretraining ({ssl_epochs} epochs)...\")\n",
        "\n",
        "            ssl_model = UnifiedSSLFramework(\n",
        "                method=config['method'],\n",
        "                channel_drop_mode=config.get('channel_mode', 'random')\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ssl_epochs)\n",
        "\n",
        "            for epoch in range(ssl_epochs):\n",
        "                metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "                scheduler.step()\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/{ssl_epochs}: Loss={metrics['loss']:.4f}\")\n",
        "\n",
        "            encoder = ssl_model.encoder\n",
        "        else:\n",
        "            encoder = ELKEncoder().to(device)\n",
        "\n",
        "        # Linear Evaluation\n",
        "        print(f\"\\n[Phase 2] Linear Evaluation ({linear_epochs} epochs)...\")\n",
        "        linear_model = LinearClassifier(encoder, num_classes=6).to(device)\n",
        "        optimizer = torch.optim.Adam(linear_model.classifier.parameters(), lr=1e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_test_acc = 0.0\n",
        "        for epoch in range(linear_epochs):\n",
        "            train_loss, train_acc = train_linear_epoch(\n",
        "                linear_model, train_loader, optimizer, criterion, device\n",
        "            )\n",
        "            test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"  Epoch {epoch+1}/{linear_epochs}: Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "        print(f\"\\n[Phase 3] Comprehensive Robustness Test...\")\n",
        "        robustness_results = evaluate_comprehensive_robustness(\n",
        "            linear_model, test_loader, device, scenarios\n",
        "        )\n",
        "\n",
        "        all_results[config['name']] = robustness_results\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'classifier_state_dict': linear_model.classifier.state_dict(),\n",
        "            'config': config,\n",
        "            'best_test_acc': best_test_acc,\n",
        "            'robustness_results': robustness_results\n",
        "        }, f\"benchmark_{config['method']}.pth\")\n",
        "\n",
        "    # Summary by Group\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ROBUSTNESS SUMMARY BY GROUP\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    groups = ['Missing-Only', 'Noise-Only', 'Combined']\n",
        "\n",
        "    for group in groups:\n",
        "        print(f\"\\n{group}:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Method':<25} {'Avg Accuracy':<15} {'Best Scenario':<20} {'Worst Scenario':<20}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for method_name, results in all_results.items():\n",
        "            group_results = {k: v['accuracy'] for k, v in results.items() if v['group'] == group}\n",
        "\n",
        "            if group_results:\n",
        "                avg_acc = np.mean(list(group_results.values()))\n",
        "                best_scenario = max(group_results.items(), key=lambda x: x[1])\n",
        "                worst_scenario = min(group_results.items(), key=lambda x: x[1])\n",
        "\n",
        "                print(f\"{method_name:<25} {avg_acc:>13.2f}% {best_scenario[0]:<20} {worst_scenario[0]:<20}\")\n",
        "\n",
        "    # Overall Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"OVERALL ROBUSTNESS COMPARISON\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Method':<25} {'Avg All':<12} {'Missing':<12} {'Noise':<12} {'Combined':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for method_name, results in all_results.items():\n",
        "        all_acc = np.mean([v['accuracy'] for v in results.values()])\n",
        "        missing_acc = np.mean([v['accuracy'] for v in results.values() if v['group'] == 'Missing-Only'])\n",
        "        noise_acc = np.mean([v['accuracy'] for v in results.values() if v['group'] == 'Noise-Only'])\n",
        "        combined_acc = np.mean([v['accuracy'] for v in results.values() if v['group'] == 'Combined'])\n",
        "\n",
        "        print(f\"{method_name:<25} {all_acc:>10.2f}% {missing_acc:>10.2f}% \"\n",
        "              f\"{noise_acc:>10.2f}% {combined_acc:>10.2f}%\")\n",
        "\n",
        "    # Detailed Table\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"DETAILED SCENARIO RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Scenario':<30}\", end='')\n",
        "    for method_name in all_results.keys():\n",
        "        print(f\"{method_name:<15}\", end='')\n",
        "    print()\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for scenario_name in scenarios.keys():\n",
        "        print(f\"{scenario_name:<30}\", end='')\n",
        "        for method_name in all_results.keys():\n",
        "            acc = all_results[method_name][scenario_name]['accuracy']\n",
        "            print(f\"{acc:>13.2f}%\", end='')\n",
        "        print()\n",
        "\n",
        "    # Save results\n",
        "    with open('benchmark_results.json', 'w') as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(f\"\\n✓ Results saved to: benchmark_results.json\")\n",
        "\n",
        "    # Generate heatmap\n",
        "    plot_robustness_heatmap(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 13. Main Entry Point\n",
        "# ============================================================================\n",
        "def main():\n",
        "    data_dir = './UCI HAR Dataset'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\\n\")\n",
        "\n",
        "    # Run comprehensive benchmark\n",
        "    results = run_comprehensive_benchmark(\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        ssl_epochs=50,\n",
        "        linear_epochs=50\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "U6L7rCNH2Tzf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}