{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG9TT9xW1DcF",
        "outputId": "dc4657d3-e978-460d-9da0-aba0b916c389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "PyTorch version: 2.8.0+cu126\n",
            "\n",
            "================================================================================\n",
            "UNIFIED TEMPORAL MASKING BENCHMARK\n",
            "================================================================================\n",
            "\n",
            "Creating fixed missing data (15% temporal masking)...\n",
            "\n",
            "================================================================================\n",
            "Method: SL-Only\n",
            "Description: Supervised learning only\n",
            "================================================================================\n",
            "\n",
            "[Phase 2] Supervised Learning (100 epochs)...\n",
            "  Epoch 20/100: Train Acc=95.99%, Test Acc=93.28%\n",
            "  Epoch 40/100: Train Acc=96.99%, Test Acc=90.84%\n",
            "  Epoch 60/100: Train Acc=97.48%, Test Acc=91.96%\n",
            "  Epoch 80/100: Train Acc=97.91%, Test Acc=92.23%\n",
            "  Epoch 100/100: Train Acc=97.97%, Test Acc=92.13%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 92.13%\n",
            "  Missing Ratio 0.10: 91.72%\n",
            "  Missing Ratio 0.30: 89.58%\n",
            "  Missing Ratio 0.50: 84.80%\n",
            "  Missing Ratio 0.70: 80.12%\n",
            "  Missing Ratio 0.90: 72.07%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: SSL w/o Missing\n",
            "Description: Standard contrastive\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=2.2475, MTC=0.0000, MICL=2.2475\n",
            "  Epoch 50/150: Loss=1.2804, MTC=0.0000, MICL=1.2804\n",
            "  Epoch 75/150: Loss=0.9131, MTC=0.0000, MICL=0.9131\n",
            "  Epoch 100/150: Loss=0.7055, MTC=0.0000, MICL=0.7055\n",
            "  Epoch 125/150: Loss=0.5876, MTC=0.0000, MICL=0.5876\n",
            "  Epoch 150/150: Loss=0.5732, MTC=0.0000, MICL=0.5732\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=57.29%, Test Acc=59.18%\n",
            "    Epoch 30/75: Train Acc=69.98%, Test Acc=70.41%\n",
            "    Epoch 45/75: Train Acc=75.73%, Test Acc=74.79%\n",
            "    Epoch 60/75: Train Acc=77.99%, Test Acc=76.59%\n",
            "    Epoch 75/75: Train Acc=77.61%, Test Acc=76.79%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=92.38%, Test Acc=88.60%\n",
            "    Epoch 10/25: Train Acc=94.70%, Test Acc=89.38%\n",
            "    Epoch 15/25: Train Acc=95.10%, Test Acc=90.06%\n",
            "    Epoch 20/25: Train Acc=95.69%, Test Acc=90.02%\n",
            "    Epoch 25/25: Train Acc=95.74%, Test Acc=89.96%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 89.96%\n",
            "  Missing Ratio 0.10: 89.07%\n",
            "  Missing Ratio 0.30: 81.61%\n",
            "  Missing Ratio 0.50: 67.36%\n",
            "  Missing Ratio 0.70: 62.91%\n",
            "  Missing Ratio 0.90: 51.68%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Random Point Drop\n",
            "Description: Non-contiguous missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=2.9802, MTC=0.0072, MICL=2.9731\n",
            "  Epoch 50/150: Loss=2.1118, MTC=0.0030, MICL=2.1088\n",
            "  Epoch 75/150: Loss=1.6971, MTC=0.0025, MICL=1.6945\n",
            "  Epoch 100/150: Loss=1.3606, MTC=0.0024, MICL=1.3581\n",
            "  Epoch 125/150: Loss=1.2637, MTC=0.0027, MICL=1.2610\n",
            "  Epoch 150/150: Loss=1.2152, MTC=0.0025, MICL=1.2126\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=60.23%, Test Acc=62.81%\n",
            "    Epoch 30/75: Train Acc=73.84%, Test Acc=76.08%\n",
            "    Epoch 45/75: Train Acc=79.45%, Test Acc=80.32%\n",
            "    Epoch 60/75: Train Acc=81.65%, Test Acc=81.78%\n",
            "    Epoch 75/75: Train Acc=82.11%, Test Acc=81.95%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=93.73%, Test Acc=88.94%\n",
            "    Epoch 10/25: Train Acc=95.31%, Test Acc=89.11%\n",
            "    Epoch 15/25: Train Acc=95.51%, Test Acc=89.31%\n",
            "    Epoch 20/25: Train Acc=95.96%, Test Acc=89.92%\n",
            "    Epoch 25/25: Train Acc=95.91%, Test Acc=89.92%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 89.92%\n",
            "  Missing Ratio 0.10: 88.80%\n",
            "  Missing Ratio 0.30: 83.20%\n",
            "  Missing Ratio 0.50: 79.33%\n",
            "  Missing Ratio 0.70: 72.51%\n",
            "  Missing Ratio 0.90: 65.35%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Random)\n",
            "Description: Random channel missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=3.9269, MTC=0.0349, MICL=3.8920\n",
            "  Epoch 50/150: Loss=2.9885, MTC=0.0188, MICL=2.9697\n",
            "  Epoch 75/150: Loss=2.4075, MTC=0.0184, MICL=2.3892\n",
            "  Epoch 100/150: Loss=2.0621, MTC=0.0163, MICL=2.0458\n",
            "  Epoch 125/150: Loss=1.8983, MTC=0.0163, MICL=1.8819\n",
            "  Epoch 150/150: Loss=1.8536, MTC=0.0168, MICL=1.8368\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=67.30%, Test Acc=70.68%\n",
            "    Epoch 30/75: Train Acc=76.24%, Test Acc=78.83%\n",
            "    Epoch 45/75: Train Acc=80.41%, Test Acc=81.78%\n",
            "    Epoch 60/75: Train Acc=81.28%, Test Acc=82.76%\n",
            "    Epoch 75/75: Train Acc=82.34%, Test Acc=82.63%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.15%, Test Acc=90.36%\n",
            "    Epoch 10/25: Train Acc=95.02%, Test Acc=90.74%\n",
            "    Epoch 15/25: Train Acc=95.47%, Test Acc=90.87%\n",
            "    Epoch 20/25: Train Acc=95.65%, Test Acc=91.04%\n",
            "    Epoch 25/25: Train Acc=95.67%, Test Acc=90.80%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 90.80%\n",
            "  Missing Ratio 0.10: 89.79%\n",
            "  Missing Ratio 0.30: 86.87%\n",
            "  Missing Ratio 0.50: 79.17%\n",
            "  Missing Ratio 0.70: 71.73%\n",
            "  Missing Ratio 0.90: 66.10%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Sensor)\n",
            "Description: Sensor-wise missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=3.8965, MTC=0.0460, MICL=3.8505\n",
            "  Epoch 50/150: Loss=2.9186, MTC=0.0402, MICL=2.8784\n",
            "  Epoch 75/150: Loss=2.3768, MTC=0.0371, MICL=2.3397\n",
            "  Epoch 100/150: Loss=2.0555, MTC=0.0353, MICL=2.0202\n",
            "  Epoch 125/150: Loss=1.9571, MTC=0.0401, MICL=1.9170\n",
            "  Epoch 150/150: Loss=1.8747, MTC=0.0410, MICL=1.8337\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=63.37%, Test Acc=66.71%\n",
            "    Epoch 30/75: Train Acc=71.78%, Test Acc=76.59%\n",
            "    Epoch 45/75: Train Acc=77.58%, Test Acc=81.07%\n",
            "    Epoch 60/75: Train Acc=79.07%, Test Acc=82.12%\n",
            "    Epoch 75/75: Train Acc=79.80%, Test Acc=82.56%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.41%, Test Acc=91.52%\n",
            "    Epoch 10/25: Train Acc=95.35%, Test Acc=91.35%\n",
            "    Epoch 15/25: Train Acc=95.76%, Test Acc=92.03%\n",
            "    Epoch 20/25: Train Acc=96.14%, Test Acc=91.99%\n",
            "    Epoch 25/25: Train Acc=96.26%, Test Acc=92.16%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 92.16%\n",
            "  Missing Ratio 0.10: 90.53%\n",
            "  Missing Ratio 0.30: 85.21%\n",
            "  Missing Ratio 0.50: 80.73%\n",
            "  Missing Ratio 0.70: 76.62%\n",
            "  Missing Ratio 0.90: 72.11%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: MTC + MICL (Ours)\n",
            "Description: Improved temporal masking\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=3.4064, MTC=0.0114, MICL=3.3950\n",
            "  Epoch 50/150: Loss=2.7050, MTC=0.0069, MICL=2.6981\n",
            "  Epoch 75/150: Loss=2.2783, MTC=0.0059, MICL=2.2724\n",
            "  Epoch 100/150: Loss=2.0503, MTC=0.0056, MICL=2.0447\n",
            "  Epoch 125/150: Loss=1.9688, MTC=0.0055, MICL=1.9633\n",
            "  Epoch 150/150: Loss=1.8649, MTC=0.0062, MICL=1.8587\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=65.18%, Test Acc=67.36%\n",
            "    Epoch 30/75: Train Acc=79.62%, Test Acc=79.91%\n",
            "    Epoch 45/75: Train Acc=84.90%, Test Acc=83.10%\n",
            "    Epoch 60/75: Train Acc=85.75%, Test Acc=83.88%\n",
            "    Epoch 75/75: Train Acc=86.13%, Test Acc=83.75%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.89%, Test Acc=88.06%\n",
            "    Epoch 10/25: Train Acc=95.65%, Test Acc=88.73%\n",
            "    Epoch 15/25: Train Acc=96.21%, Test Acc=89.82%\n",
            "    Epoch 20/25: Train Acc=96.00%, Test Acc=90.06%\n",
            "    Epoch 25/25: Train Acc=96.29%, Test Acc=90.06%\n",
            "\n",
            "[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\n",
            "  Missing Ratio 0.00: 90.06%\n",
            "  Missing Ratio 0.10: 89.45%\n",
            "  Missing Ratio 0.30: 85.88%\n",
            "  Missing Ratio 0.50: 80.59%\n",
            "  Missing Ratio 0.70: 75.43%\n",
            "  Missing Ratio 0.90: 69.73%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.01 ms/sample\n",
            "\n",
            "================================================================================\n",
            "UNIFIED TEMPORAL BENCHMARK SUMMARY\n",
            "================================================================================\n",
            "SL-Only                     92.1%   91.7%   89.6%   84.8%   80.1%   72.1%    163K    3.8M    0.0\n",
            "SSL w/o Missing             90.0%   89.1%   81.6%   67.4%   62.9%   51.7%    163K    3.8M    0.0\n",
            "Random Point Drop           89.9%   88.8%   83.2%   79.3%   72.5%   65.4%    163K    3.8M    0.0\n",
            "Channel Drop (Random)       90.8%   89.8%   86.9%   79.2%   71.7%   66.1%    163K    3.8M    0.0\n",
            "Channel Drop (Sensor)       92.2%   90.5%   85.2%   80.7%   76.6%   72.1%    163K    3.8M    0.0\n",
            "MTC + MICL (Ours)           90.1%   89.4%   85.9%   80.6%   75.4%   69.7%    163K    3.8M    0.0\n",
            "\n",
            "âœ“ Results saved to: unified_temporal_benchmark_results.json\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_uci_har_raw(dataset_path):\n",
        "    SIGNALS = [\"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
        "    X_train_list, X_test_list = [], []\n",
        "    for signal in SIGNALS:\n",
        "        train_file = os.path.join(dataset_path, f\"{signal}_train.txt\")\n",
        "        test_file = os.path.join(dataset_path, f\"{signal}_test.txt\")\n",
        "        X_train_list.append(np.loadtxt(train_file, dtype=np.float32))\n",
        "        X_test_list.append(np.loadtxt(test_file, dtype=np.float32))\n",
        "    X_train = np.transpose(np.stack(X_train_list, axis=-1), (0, 2, 1))\n",
        "    X_test = np.transpose(np.stack(X_test_list, axis=-1), (0, 2, 1))\n",
        "    y_train = np.loadtxt(os.path.join(dataset_path, 'y_train.txt'), dtype=int) - 1\n",
        "    y_test = np.loadtxt(os.path.join(dataset_path, 'y_test.txt'), dtype=int) - 1\n",
        "    activity_names = ['Walking', 'Walking Upstairs', 'Walking Downstairs', 'Sitting', 'Standing', 'Laying']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "    X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
        "    X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
        "\n",
        "    return X_train_scaled, y_train, X_test_scaled, y_test, activity_names\n",
        "\n",
        "def create_fixed_missing_data(X, missing_ratio=0.15, mask_length_range=(5, 15), noise_std=0.1, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    N, C, T = X.shape\n",
        "    X_missing = X.copy()\n",
        "\n",
        "    for n in range(N):\n",
        "        mask = np.ones(T, dtype=bool)\n",
        "        num_to_mask = int(T * missing_ratio)\n",
        "\n",
        "        while num_to_mask > 0:\n",
        "            length = np.random.randint(mask_length_range[0], min(mask_length_range[1], T) + 1)\n",
        "            length = min(length, num_to_mask, T)\n",
        "            start = np.random.randint(0, T - length + 1)\n",
        "            mask[start:start + length] = False\n",
        "            num_to_mask -= length\n",
        "\n",
        "        noise = np.random.randn(C, T).astype(np.float32) * noise_std\n",
        "        for c in range(C):\n",
        "            X_missing[n, c, ~mask] = noise[c, ~mask]\n",
        "\n",
        "    return X_missing\n",
        "\n",
        "class ImprovedTemporalMasking:\n",
        "    def __init__(self, mask_ratio=0.15, mask_length_range=(5, 15), noise_std=0.1):\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_length_range = mask_length_range\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        max_len = min(self.mask_length_range[1], T)\n",
        "        min_len = min(self.mask_length_range[0], T)\n",
        "\n",
        "        for b in range(B):\n",
        "            num_to_mask = int(T * self.mask_ratio)\n",
        "            while num_to_mask > 0:\n",
        "                length = np.random.randint(min_len, max_len + 1)\n",
        "                length = min(length, num_to_mask, T)\n",
        "                start = np.random.randint(0, T - length + 1)\n",
        "                mask[b, start:start + length] = False\n",
        "                num_to_mask -= length\n",
        "\n",
        "        masked_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        masked_x[:, :, ~mask[0]] = noise[:, :, ~mask[0]]\n",
        "\n",
        "        return masked_x, mask\n",
        "\n",
        "class ImprovedRandomPointDrop:\n",
        "    def __init__(self, drop_ratio=0.15, noise_std=0.1):\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        mask = torch.rand(B, T, device=x.device) > self.drop_ratio\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        dropped_x[:, :, ~mask[0]] = noise[:, :, ~mask[0]]\n",
        "\n",
        "        return dropped_x, mask\n",
        "\n",
        "class ImprovedChannelDrop:\n",
        "    def __init__(self, drop_prob=0.2, mode='random', noise_std=0.1):\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mode = mode\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "\n",
        "        if self.mode == 'random':\n",
        "            channel_mask = torch.rand(B, C, device=x.device) > self.drop_prob\n",
        "        elif self.mode == 'axis':\n",
        "            axis_groups = [[0,3,6], [1,4,7], [2,5,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in axis_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "        elif self.mode == 'sensor':\n",
        "            sensor_groups = [[0,1,2], [3,4,5], [6,7,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in sensor_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        for b in range(B):\n",
        "            for c in range(C):\n",
        "                if not channel_mask[b, c]:\n",
        "                    dropped_x[b, c, :] = noise[b, c, :]\n",
        "\n",
        "        temporal_mask = channel_mask.any(dim=1, keepdim=True).expand(B, T)\n",
        "        return dropped_x, temporal_mask\n",
        "\n",
        "class StandardAugmentation:\n",
        "    def __init__(self, noise_std=0.02, scale_range=(0.9, 1.1)):\n",
        "        self.noise_std = noise_std\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        augmented_x = x + noise\n",
        "        scale = torch.empty(B, C, 1, device=x.device).uniform_(*self.scale_range)\n",
        "        augmented_x = augmented_x * scale\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "        return augmented_x, mask\n",
        "\n",
        "class ELKBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
        "        super().__init__()\n",
        "        self.deploy = deploy\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding_large1 = kernel_size // 2\n",
        "        kernel_size_large2 = kernel_size - 2\n",
        "        padding_large2 = kernel_size_large2 // 2\n",
        "        kernel_size_small1 = 5\n",
        "        padding_small1 = kernel_size_small1 // 2\n",
        "        kernel_size_small2 = 3\n",
        "        padding_small2 = kernel_size_small2 // 2\n",
        "\n",
        "        if deploy:\n",
        "            self.reparam_conv = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=True\n",
        "            )\n",
        "        else:\n",
        "            self.dw_large1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_large2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_large2,\n",
        "                padding=padding_large2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small1,\n",
        "                padding=padding_small1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small2,\n",
        "                padding=padding_small2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
        "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deploy:\n",
        "            x = self.reparam_conv(x)\n",
        "        else:\n",
        "            x1 = self.bn_large1(self.dw_large1(x))\n",
        "            x2 = self.bn_large2(self.dw_large2(x))\n",
        "            x3 = self.bn_small1(self.dw_small1(x))\n",
        "            x4 = self.bn_small2(self.dw_small2(x))\n",
        "            x5 = self.bn_id(x)\n",
        "            x = x1 + x2 + x3 + x4 + x5\n",
        "        x = self.activation(x)\n",
        "        return self.pointwise(x)\n",
        "\n",
        "class LightweightELKBackbone(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=1, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.elk_block = ELKBlock(d_model, d_model, kernel_size=kernel_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out_channels = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.elk_block(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "def improved_masked_pooling(h, mask, pool_type='attention'):\n",
        "    if pool_type == 'attention':\n",
        "        B, C, T = h.shape\n",
        "        mask_float = mask.float().unsqueeze(1)\n",
        "\n",
        "        attention_weights = torch.softmax(h.mean(dim=1, keepdim=True), dim=-1)\n",
        "        attention_weights = attention_weights * mask_float\n",
        "        attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        pooled = (h * attention_weights).sum(dim=-1)\n",
        "        return pooled\n",
        "\n",
        "    elif pool_type == 'weighted_avg':\n",
        "        mask_float = mask.unsqueeze(1).float()\n",
        "        numerator = (h * mask_float).sum(dim=-1)\n",
        "        denominator = mask_float.sum(dim=-1).clamp_min(1e-6)\n",
        "        return numerator / denominator\n",
        "\n",
        "    else:\n",
        "        return h.mean(dim=-1)\n",
        "\n",
        "class ImprovedELKEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=1,\n",
        "                 kernel_size=31, output_dim=256, dropout=0.1, pool_type='attention'):\n",
        "        super().__init__()\n",
        "        self.backbone = LightweightELKBackbone(in_channels, d_model, num_layers, kernel_size, dropout)\n",
        "        self.pool_type = pool_type\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(d_model, output_dim),\n",
        "            nn.LayerNorm(output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        h = self.backbone(x)\n",
        "        if mask is not None:\n",
        "            h = improved_masked_pooling(h, mask, self.pool_type)\n",
        "        else:\n",
        "            h = h.mean(dim=-1)\n",
        "        z = self.projection(h)\n",
        "        z = F.normalize(z, dim=1)\n",
        "        return z\n",
        "\n",
        "class ImprovedMTCLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1, use_cosine=True):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.use_cosine = use_cosine\n",
        "\n",
        "    def forward(self, z_clean, z_masked):\n",
        "        if self.use_cosine:\n",
        "            sim = F.cosine_similarity(z_clean, z_masked, dim=1)\n",
        "            loss = -torch.log(torch.sigmoid(sim / self.temperature)).mean()\n",
        "        else:\n",
        "            loss = F.mse_loss(z_clean, z_masked)\n",
        "        return loss\n",
        "\n",
        "class ImprovedNTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07, use_hard_negatives=True):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.use_hard_negatives = use_hard_negatives\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        device = z1.device\n",
        "\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        sim_11 = (z1 @ z1.T) / self.temperature\n",
        "        sim_22 = (z2 @ z2.T) / self.temperature\n",
        "        sim_12 = (z1 @ z2.T) / self.temperature\n",
        "        sim_21 = sim_12.T\n",
        "\n",
        "        mask = torch.eye(B, device=device, dtype=torch.bool)\n",
        "        sim_11 = sim_11.masked_fill(mask, -9e15)\n",
        "        sim_22 = sim_22.masked_fill(mask, -9e15)\n",
        "\n",
        "        if self.use_hard_negatives:\n",
        "            hard_neg_weight = 2.0\n",
        "            sim_11 = sim_11 * hard_neg_weight\n",
        "            sim_22 = sim_22 * hard_neg_weight\n",
        "\n",
        "        logits_1 = torch.cat([sim_12, sim_11], dim=1)\n",
        "        logits_2 = torch.cat([sim_21, sim_22], dim=1)\n",
        "        labels = torch.arange(B, device=device)\n",
        "\n",
        "        loss_1 = F.cross_entropy(logits_1, labels)\n",
        "        loss_2 = F.cross_entropy(logits_2, labels)\n",
        "\n",
        "        return 0.5 * (loss_1 + loss_2)\n",
        "\n",
        "class ImprovedSSLFramework(nn.Module):\n",
        "    def __init__(self, method='mtc_micl', in_channels=9, d_model=128,\n",
        "                 num_layers=1, kernel_size=31, output_dim=256, dropout=0.1,\n",
        "                 mask_ratio=0.15, temperature=0.07,\n",
        "                 lambda_mtc=1.0, lambda_micl=1.0, channel_drop_mode='random',\n",
        "                 pool_type='attention'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.method = method\n",
        "        self.encoder = ImprovedELKEncoder(in_channels, d_model, num_layers,\n",
        "                                        kernel_size, output_dim, dropout, pool_type)\n",
        "\n",
        "        if method == 'sl_only':\n",
        "            self.augmentation = None\n",
        "        elif method == 'ssl_wo_missing':\n",
        "            self.augmentation = StandardAugmentation()\n",
        "        elif method == 'random_point_drop':\n",
        "            self.augmentation = ImprovedRandomPointDrop(drop_ratio=mask_ratio)\n",
        "        elif method == 'channel_drop':\n",
        "            self.augmentation = ImprovedChannelDrop(drop_prob=mask_ratio, mode=channel_drop_mode)\n",
        "        elif method == 'mtc_micl':\n",
        "            self.augmentation = ImprovedTemporalMasking(mask_ratio=mask_ratio)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "        self.mtc_loss = ImprovedMTCLoss(temperature=0.1)\n",
        "        self.micl_loss = ImprovedNTXentLoss(temperature=temperature)\n",
        "        self.lambda_mtc = lambda_mtc\n",
        "        self.lambda_micl = lambda_micl\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.method == 'sl_only':\n",
        "            return torch.tensor(0.0, device=x.device), {'total': 0.0, 'mtc': 0.0, 'micl': 0.0}\n",
        "\n",
        "        z_clean = self.encoder(x, mask=None)\n",
        "        x_aug, mask = self.augmentation(x)\n",
        "        z_aug = self.encoder(x_aug, mask=mask)\n",
        "\n",
        "        if self.method == 'ssl_wo_missing':\n",
        "            loss_mtc = torch.tensor(0.0, device=x.device)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = loss_micl\n",
        "        else:\n",
        "            loss_mtc = self.mtc_loss(z_clean, z_aug)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = self.lambda_mtc * loss_mtc + self.lambda_micl * loss_micl\n",
        "\n",
        "        losses_dict = {\n",
        "            'total': loss.item(),\n",
        "            'mtc': loss_mtc.item(),\n",
        "            'micl': loss_micl.item()\n",
        "        }\n",
        "\n",
        "        return loss, losses_dict\n",
        "\n",
        "    def get_representation(self, x, mask=None):\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(x, mask=mask)\n",
        "\n",
        "class TwoStageLinearClassifier(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        self._freeze_encoder()\n",
        "\n",
        "    def _freeze_encoder(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def _unfreeze_encoder(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x, mask=None)\n",
        "        return self.classifier(z)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_flops(model, input_shape=(1, 9, 128), device='cuda'):\n",
        "    model.eval()\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "\n",
        "    def conv_flop_count(input_shape, weight_shape, stride=1, padding=0):\n",
        "        batch_size, in_channels, input_length = input_shape\n",
        "        out_channels, in_channels, kernel_size = weight_shape\n",
        "        output_length = (input_length + 2 * padding - kernel_size) // stride + 1\n",
        "        return batch_size * out_channels * output_length * in_channels * kernel_size\n",
        "\n",
        "    def linear_flop_count(input_features, output_features):\n",
        "        return input_features * output_features\n",
        "\n",
        "    total_flops = 0\n",
        "\n",
        "    def flop_hook(module, input, output):\n",
        "        nonlocal total_flops\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            input_shape = input[0].shape\n",
        "            weight_shape = module.weight.shape\n",
        "            stride = module.stride[0] if isinstance(module.stride, tuple) else module.stride\n",
        "            padding = module.padding[0] if isinstance(module.padding, tuple) else module.padding\n",
        "            flops = conv_flop_count(input_shape, weight_shape, stride, padding)\n",
        "            total_flops += flops\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            input_features = input[0].shape[-1]\n",
        "            output_features = module.out_features\n",
        "            batch_size = input[0].shape[0]\n",
        "            flops = batch_size * linear_flop_count(input_features, output_features)\n",
        "            total_flops += flops\n",
        "\n",
        "    hooks = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
        "            hooks.append(module.register_forward_hook(flop_hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
        "    model.eval()\n",
        "    total_time = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            x = x.to(device)\n",
        "\n",
        "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
        "            start_time = time.time()\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
        "            end_time = time.time()\n",
        "\n",
        "            total_time += end_time - start_time\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_time / total_samples * 1000\n",
        "\n",
        "def train_ssl_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mtc = 0\n",
        "    total_micl = 0\n",
        "\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, losses_dict = model(x)\n",
        "        if loss.item() > 0:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "        total_loss += losses_dict['total']\n",
        "        total_mtc += losses_dict['mtc']\n",
        "        total_micl += losses_dict['micl']\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'mtc': total_mtc / num_batches,\n",
        "        'micl': total_micl / num_batches\n",
        "    }\n",
        "\n",
        "def train_two_stage_linear(model, train_loader, test_loader, device,\n",
        "                          stage1_epochs=75, stage2_epochs=25,\n",
        "                          stage1_lr=1e-3, stage2_lr=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_test_acc = 0.0\n",
        "\n",
        "    print(f\"  Stage 1: Frozen encoder + classifier training ({stage1_epochs} epochs)\")\n",
        "    model._freeze_encoder()\n",
        "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=stage1_lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage1_epochs)\n",
        "\n",
        "    for epoch in range(stage1_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "        test_acc = evaluate_linear(model, test_loader, device)\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 15 == 0:\n",
        "            print(f\"    Epoch {epoch+1}/{stage1_epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"  Stage 2: Fine-tuning entire model ({stage2_epochs} epochs)\")\n",
        "    model._unfreeze_encoder()\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': model.encoder.parameters(), 'lr': stage2_lr},\n",
        "        {'params': model.classifier.parameters(), 'lr': stage1_lr}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_groups, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage2_epochs)\n",
        "\n",
        "    for epoch in range(stage2_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "        test_acc = evaluate_linear(model, test_loader, device)\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"    Epoch {epoch+1}/{stage2_epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "    return best_test_acc\n",
        "\n",
        "def evaluate_linear(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "def evaluate_missing_robustness_unified(model, dataloader, device, missing_ratios=[0.0, 0.15, 0.3, 0.5]):\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for ratio in missing_ratios:\n",
        "        masking = ImprovedTemporalMasking(mask_ratio=ratio)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in dataloader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                if ratio > 0:\n",
        "                    x_masked, mask = masking(x)\n",
        "                    logits = model(x_masked)\n",
        "                else:\n",
        "                    logits = model(x)\n",
        "\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        results[ratio] = 100.0 * correct / total\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_unified_temporal_benchmark(data_dir, device, ssl_epochs=150, batch_size=128):\n",
        "    print(\"=\"*80)\n",
        "    print(\"UNIFIED TEMPORAL MASKING BENCHMARK\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_train, y_train, X_test, y_test, activity_names = load_uci_har_raw(data_dir)\n",
        "\n",
        "    print(\"\\nCreating fixed missing data (15% temporal masking)...\")\n",
        "    X_train_missing = create_fixed_missing_data(X_train, missing_ratio=0.15, seed=42)\n",
        "    X_test_missing = create_fixed_missing_data(X_test, missing_ratio=0.15, seed=43)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train_missing), torch.LongTensor(y_train))\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(X_test_missing), torch.LongTensor(y_test))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    methods = [\n",
        "        {'name': 'SL-Only', 'method': 'sl_only', 'desc': 'Supervised learning only'},\n",
        "        {'name': 'SSL w/o Missing', 'method': 'ssl_wo_missing', 'desc': 'Standard contrastive'},\n",
        "        {'name': 'Random Point Drop', 'method': 'random_point_drop', 'desc': 'Non-contiguous missing'},\n",
        "        {'name': 'Channel Drop (Random)', 'method': 'channel_drop', 'desc': 'Random channel missing', 'channel_mode': 'random'},\n",
        "        {'name': 'Channel Drop (Sensor)', 'method': 'channel_drop', 'desc': 'Sensor-wise missing', 'channel_mode': 'sensor'},\n",
        "        {'name': 'MTC + MICL (Ours)', 'method': 'mtc_micl', 'desc': 'Improved temporal masking'},\n",
        "    ]\n",
        "\n",
        "    results = defaultdict(dict)\n",
        "\n",
        "    for config in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Method: {config['name']}\")\n",
        "        print(f\"Description: {config['desc']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        if config['method'] != 'sl_only':\n",
        "            print(f\"\\n[Phase 1] SSL Pretraining ({ssl_epochs} epochs)...\")\n",
        "\n",
        "            ssl_model = ImprovedSSLFramework(\n",
        "                method=config['method'],\n",
        "                channel_drop_mode=config.get('channel_mode', 'random'),\n",
        "                mask_ratio=0.15,\n",
        "                num_layers=1\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ssl_epochs)\n",
        "\n",
        "            for epoch in range(ssl_epochs):\n",
        "                metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "                scheduler.step()\n",
        "\n",
        "                if (epoch + 1) % 25 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/{ssl_epochs}: Loss={metrics['loss']:.4f}, \"\n",
        "                          f\"MTC={metrics['mtc']:.4f}, MICL={metrics['micl']:.4f}\")\n",
        "\n",
        "            encoder = ssl_model.encoder\n",
        "        else:\n",
        "            encoder = ImprovedELKEncoder(num_layers=1).to(device)\n",
        "\n",
        "        if config['method'] == 'sl_only':\n",
        "            print(f\"\\n[Phase 2] Supervised Learning (100 epochs)...\")\n",
        "            linear_model = TwoStageLinearClassifier(encoder, num_classes=6).to(device)\n",
        "            linear_model._unfreeze_encoder()\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.AdamW(linear_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "            best_test_acc = 0.0\n",
        "            for epoch in range(100):\n",
        "                linear_model.train()\n",
        "                total_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for x, y in train_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = linear_model(x)\n",
        "                    loss = criterion(logits, y)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(linear_model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y).sum().item()\n",
        "                    total += y.size(0)\n",
        "\n",
        "                scheduler.step()\n",
        "                train_acc = 100.0 * correct / total\n",
        "                test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "                best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "                if (epoch + 1) % 20 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/100: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "        else:\n",
        "            print(f\"\\n[Phase 2] Two-Stage Linear Evaluation...\")\n",
        "            linear_model = TwoStageLinearClassifier(encoder, num_classes=6).to(device)\n",
        "\n",
        "            best_test_acc = train_two_stage_linear(\n",
        "                linear_model, train_loader, test_loader, device,\n",
        "                stage1_epochs=75, stage2_epochs=25,\n",
        "                stage1_lr=1e-3, stage2_lr=1e-5\n",
        "            )\n",
        "\n",
        "        results[config['name']]['test_accuracy'] = best_test_acc\n",
        "\n",
        "        total_params = count_parameters(linear_model)\n",
        "        encoder_params = count_parameters(linear_model.encoder)\n",
        "        total_flops = calculate_flops(linear_model, input_shape=(1, 9, 128), device=device)\n",
        "        inference_time = measure_inference_time(linear_model, test_loader, device)\n",
        "\n",
        "        results[config['name']]['total_params'] = total_params\n",
        "        results[config['name']]['encoder_params'] = encoder_params\n",
        "        results[config['name']]['total_flops'] = total_flops\n",
        "        results[config['name']]['inference_time_ms'] = inference_time\n",
        "\n",
        "        print(f\"\\n[Phase 3] Missing Robustness Test (Unified Temporal Masking)...\")\n",
        "        missing_results = evaluate_missing_robustness_unified(\n",
        "            linear_model, test_loader, device,\n",
        "            missing_ratios=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "        )\n",
        "        results[config['name']]['missing_robustness'] = missing_results\n",
        "\n",
        "        for ratio, acc in missing_results.items():\n",
        "            print(f\"  Missing Ratio {ratio:.2f}: {acc:.2f}%\")\n",
        "\n",
        "        print(f\"\\n[Model Statistics]\")\n",
        "        print(f\"  Total Parameters: {total_params:,}\")\n",
        "        print(f\"  Encoder Parameters: {encoder_params:,}\")\n",
        "        print(f\"  Total FLOPs: {total_flops/1e6:.2f}M\")\n",
        "        print(f\"  Inference Time: {inference_time:.2f} ms/sample\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"UNIFIED TEMPORAL BENCHMARK SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    for method_name, result in results.items():\n",
        "        clean_acc = result['missing_robustness'][0.0]\n",
        "        miss_10 = result['missing_robustness'].get(0.1, 0.0)\n",
        "        miss_30 = result['missing_robustness'].get(0.3, 0.0)\n",
        "        miss_50 = result['missing_robustness'].get(0.5, 0.0)\n",
        "        miss_70 = result['missing_robustness'].get(0.7, 0.0)\n",
        "        miss_90 = result['missing_robustness'].get(0.9, 0.0)\n",
        "\n",
        "        params = result['total_params'] // 1000\n",
        "        flops = result['total_flops'] / 1e6\n",
        "        inf_time = result['inference_time_ms']\n",
        "\n",
        "        print(f\"{method_name:<25} {clean_acc:>6.1f}% {miss_10:>6.1f}% {miss_30:>6.1f}% \"\n",
        "              f\"{miss_50:>6.1f}% {miss_70:>6.1f}% {miss_90:>6.1f}% {params:>6}K {flops:>6.1f}M {inf_time:>6.1f}\")\n",
        "\n",
        "    with open('unified_temporal_benchmark_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nâœ“ Results saved to: unified_temporal_benchmark_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    data_dir = '/content/'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\\n\")\n",
        "\n",
        "    results = run_unified_temporal_benchmark(\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        ssl_epochs=150,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nimS6uZr4RnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}