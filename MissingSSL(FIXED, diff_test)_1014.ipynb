{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torchprofile torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIUri3aRMdE_",
        "outputId": "7e1a4bc8-332e-4fb5-c594-071a5092c4df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.4->torchprofile) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->torchprofile) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->torchprofile) (3.0.3)\n",
            "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: torchprofile\n",
            "Successfully installed torchprofile-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuN7nrI-KeG3",
        "outputId": "14120a87-b770-4976-f79d-2b455ff53564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "PyTorch version: 2.8.0+cu126\n",
            "\n",
            "================================================================================\n",
            "IMPROVED BENCHMARK: Lightweight ELK with Two-Stage Training\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Method: SL-Only\n",
            "Description: Supervised learning only\n",
            "================================================================================\n",
            "\n",
            "[Phase 2] Supervised Learning (100 epochs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 20/100: Train Acc=96.18%, Test Acc=91.52%\n",
            "  Epoch 40/100: Train Acc=96.68%, Test Acc=92.47%\n",
            "  Epoch 60/100: Train Acc=97.05%, Test Acc=92.77%\n",
            "  Epoch 80/100: Train Acc=97.59%, Test Acc=92.57%\n",
            "  Epoch 100/100: Train Acc=97.95%, Test Acc=93.04%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 93.04%\n",
            "  Missing Ratio 0.15: 90.09%\n",
            "  Missing Ratio 0.30: 83.54%\n",
            "  Missing Ratio 0.50: 70.55%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.02 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: SSL w/o Missing\n",
            "Description: Standard contrastive\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=1.9001, MTC=0.0000, MICL=1.9001\n",
            "  Epoch 50/150: Loss=1.4677, MTC=0.0000, MICL=1.4677\n",
            "  Epoch 75/150: Loss=1.2418, MTC=0.0000, MICL=1.2418\n",
            "  Epoch 100/150: Loss=1.0426, MTC=0.0000, MICL=1.0426\n",
            "  Epoch 125/150: Loss=0.9185, MTC=0.0000, MICL=0.9185\n",
            "  Epoch 150/150: Loss=0.9490, MTC=0.0000, MICL=0.9490\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=73.12%, Test Acc=75.36%\n",
            "    Epoch 30/75: Train Acc=83.87%, Test Acc=81.00%\n",
            "    Epoch 45/75: Train Acc=87.62%, Test Acc=81.95%\n",
            "    Epoch 60/75: Train Acc=88.62%, Test Acc=82.29%\n",
            "    Epoch 75/75: Train Acc=88.60%, Test Acc=82.42%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.67%, Test Acc=87.28%\n",
            "    Epoch 10/25: Train Acc=95.47%, Test Acc=87.89%\n",
            "    Epoch 15/25: Train Acc=96.08%, Test Acc=88.70%\n",
            "    Epoch 20/25: Train Acc=96.45%, Test Acc=88.46%\n",
            "    Epoch 25/25: Train Acc=96.64%, Test Acc=88.84%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 88.84%\n",
            "  Missing Ratio 0.15: 86.05%\n",
            "  Missing Ratio 0.30: 81.91%\n",
            "  Missing Ratio 0.50: 74.65%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.02 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Random Point Drop\n",
            "Description: Non-contiguous missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=2.2392, MTC=0.0035, MICL=2.2357\n",
            "  Epoch 50/150: Loss=1.5697, MTC=0.0021, MICL=1.5677\n",
            "  Epoch 75/150: Loss=1.2851, MTC=0.0019, MICL=1.2832\n",
            "  Epoch 100/150: Loss=1.1411, MTC=0.0019, MICL=1.1392\n",
            "  Epoch 125/150: Loss=1.0505, MTC=0.0019, MICL=1.0485\n",
            "  Epoch 150/150: Loss=0.9554, MTC=0.0021, MICL=0.9533\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=75.65%, Test Acc=76.89%\n",
            "    Epoch 30/75: Train Acc=85.75%, Test Acc=83.31%\n",
            "    Epoch 45/75: Train Acc=88.82%, Test Acc=84.22%\n",
            "    Epoch 60/75: Train Acc=89.59%, Test Acc=84.66%\n",
            "    Epoch 75/75: Train Acc=89.76%, Test Acc=85.04%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.37%, Test Acc=89.38%\n",
            "    Epoch 10/25: Train Acc=95.87%, Test Acc=90.23%\n",
            "    Epoch 15/25: Train Acc=96.16%, Test Acc=90.16%\n",
            "    Epoch 20/25: Train Acc=96.38%, Test Acc=90.19%\n",
            "    Epoch 25/25: Train Acc=96.42%, Test Acc=90.53%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 90.53%\n",
            "  Missing Ratio 0.15: 87.82%\n",
            "  Missing Ratio 0.30: 82.19%\n",
            "  Missing Ratio 0.50: 68.54%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.02 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Random)\n",
            "Description: Random channel missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=3.4650, MTC=0.0421, MICL=3.4229\n",
            "  Epoch 50/150: Loss=2.8873, MTC=0.0295, MICL=2.8579\n",
            "  Epoch 75/150: Loss=2.5676, MTC=0.0260, MICL=2.5416\n",
            "  Epoch 100/150: Loss=2.3549, MTC=0.0256, MICL=2.3293\n",
            "  Epoch 125/150: Loss=2.2063, MTC=0.0254, MICL=2.1809\n",
            "  Epoch 150/150: Loss=2.1111, MTC=0.0269, MICL=2.0842\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=81.20%, Test Acc=85.78%\n",
            "    Epoch 30/75: Train Acc=89.62%, Test Acc=88.53%\n",
            "    Epoch 45/75: Train Acc=92.11%, Test Acc=88.87%\n",
            "    Epoch 60/75: Train Acc=92.32%, Test Acc=89.28%\n",
            "    Epoch 75/75: Train Acc=92.37%, Test Acc=89.21%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=95.61%, Test Acc=91.41%\n",
            "    Epoch 10/25: Train Acc=95.89%, Test Acc=92.16%\n",
            "    Epoch 15/25: Train Acc=96.30%, Test Acc=92.33%\n",
            "    Epoch 20/25: Train Acc=96.68%, Test Acc=92.60%\n",
            "    Epoch 25/25: Train Acc=96.68%, Test Acc=92.64%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 92.64%\n",
            "  Missing Ratio 0.15: 90.77%\n",
            "  Missing Ratio 0.30: 86.90%\n",
            "  Missing Ratio 0.50: 79.17%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.03 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: Channel Drop (Sensor)\n",
            "Description: Sensor-wise missing\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=3.2261, MTC=0.0457, MICL=3.1804\n",
            "  Epoch 50/150: Loss=2.5296, MTC=0.0403, MICL=2.4893\n",
            "  Epoch 75/150: Loss=2.2443, MTC=0.0366, MICL=2.2078\n",
            "  Epoch 100/150: Loss=2.0586, MTC=0.0391, MICL=2.0195\n",
            "  Epoch 125/150: Loss=1.8860, MTC=0.0379, MICL=1.8481\n",
            "  Epoch 150/150: Loss=1.9928, MTC=0.0396, MICL=1.9532\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=73.64%, Test Acc=76.99%\n",
            "    Epoch 30/75: Train Acc=86.58%, Test Acc=85.61%\n",
            "    Epoch 45/75: Train Acc=89.54%, Test Acc=86.94%\n",
            "    Epoch 60/75: Train Acc=90.47%, Test Acc=87.58%\n",
            "    Epoch 75/75: Train Acc=90.97%, Test Acc=87.34%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=95.54%, Test Acc=90.74%\n",
            "    Epoch 10/25: Train Acc=96.08%, Test Acc=91.58%\n",
            "    Epoch 15/25: Train Acc=96.52%, Test Acc=91.58%\n",
            "    Epoch 20/25: Train Acc=96.87%, Test Acc=91.89%\n",
            "    Epoch 25/25: Train Acc=96.83%, Test Acc=92.03%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 92.03%\n",
            "  Missing Ratio 0.15: 90.06%\n",
            "  Missing Ratio 0.30: 87.68%\n",
            "  Missing Ratio 0.50: 79.40%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.02 ms/sample\n",
            "\n",
            "================================================================================\n",
            "Method: MTC + MICL (Ours)\n",
            "Description: Improved temporal masking\n",
            "================================================================================\n",
            "\n",
            "[Phase 1] SSL Pretraining (150 epochs)...\n",
            "  Epoch 25/150: Loss=2.3224, MTC=0.0040, MICL=2.3184\n",
            "  Epoch 50/150: Loss=1.7532, MTC=0.0032, MICL=1.7500\n",
            "  Epoch 75/150: Loss=1.5485, MTC=0.0029, MICL=1.5457\n",
            "  Epoch 100/150: Loss=1.3806, MTC=0.0029, MICL=1.3777\n",
            "  Epoch 125/150: Loss=1.2377, MTC=0.0030, MICL=1.2347\n",
            "  Epoch 150/150: Loss=1.2472, MTC=0.0031, MICL=1.2441\n",
            "\n",
            "[Phase 2] Two-Stage Linear Evaluation...\n",
            "  Stage 1: Frozen encoder + classifier training (75 epochs)\n",
            "    Epoch 15/75: Train Acc=77.09%, Test Acc=80.01%\n",
            "    Epoch 30/75: Train Acc=87.50%, Test Acc=84.39%\n",
            "    Epoch 45/75: Train Acc=89.62%, Test Acc=84.87%\n",
            "    Epoch 60/75: Train Acc=90.13%, Test Acc=85.24%\n",
            "    Epoch 75/75: Train Acc=91.24%, Test Acc=85.41%\n",
            "  Stage 2: Fine-tuning entire model (25 epochs)\n",
            "    Epoch 5/25: Train Acc=94.87%, Test Acc=89.45%\n",
            "    Epoch 10/25: Train Acc=96.34%, Test Acc=90.02%\n",
            "    Epoch 15/25: Train Acc=96.49%, Test Acc=90.26%\n",
            "    Epoch 20/25: Train Acc=96.80%, Test Acc=90.26%\n",
            "    Epoch 25/25: Train Acc=96.68%, Test Acc=90.36%\n",
            "\n",
            "[Phase 3] Missing Robustness Test...\n",
            "  Missing Ratio 0.00: 90.36%\n",
            "  Missing Ratio 0.15: 88.67%\n",
            "  Missing Ratio 0.30: 85.71%\n",
            "  Missing Ratio 0.50: 77.91%\n",
            "\n",
            "[Model Statistics]\n",
            "  Total Parameters: 163,334\n",
            "  Encoder Parameters: 129,664\n",
            "  Total FLOPs: 3.79M\n",
            "  Inference Time: 0.03 ms/sample\n",
            "\n",
            "================================================================================\n",
            "IMPROVED BENCHMARK SUMMARY\n",
            "================================================================================\n",
            "Method                    Clean Acc  15% Miss   30% Miss   50% Miss   Params   FLOPs    Time(ms)\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "SL-Only                       93.0%     90.1%     83.5%     70.5%    163K    3.8M    0.0\n",
            "SSL w/o Missing               88.8%     86.1%     81.9%     74.7%    163K    3.8M    0.0\n",
            "Random Point Drop             90.5%     87.8%     82.2%     68.5%    163K    3.8M    0.0\n",
            "Channel Drop (Random)         92.6%     90.8%     86.9%     79.2%    163K    3.8M    0.0\n",
            "Channel Drop (Sensor)         92.0%     90.1%     87.7%     79.4%    163K    3.8M    0.0\n",
            "MTC + MICL (Ours)             90.4%     88.7%     85.7%     77.9%    163K    3.8M    0.0\n",
            "\n",
            "✓ Results saved to: improved_benchmark_results.json\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_uci_har_raw(dataset_path):\n",
        "    SIGNALS = [\"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
        "    X_train_list, X_test_list = [], []\n",
        "    for signal in SIGNALS:\n",
        "        train_file = os.path.join(dataset_path, f\"{signal}_train.txt\")\n",
        "        test_file = os.path.join(dataset_path, f\"{signal}_test.txt\")\n",
        "        X_train_list.append(np.loadtxt(train_file, dtype=np.float32))\n",
        "        X_test_list.append(np.loadtxt(test_file, dtype=np.float32))\n",
        "    X_train = np.transpose(np.stack(X_train_list, axis=-1), (0, 2, 1))\n",
        "    X_test = np.transpose(np.stack(X_test_list, axis=-1), (0, 2, 1))\n",
        "    y_train = np.loadtxt(os.path.join(dataset_path, 'y_train.txt'), dtype=int) - 1\n",
        "    y_test = np.loadtxt(os.path.join(dataset_path, 'y_test.txt'), dtype=int) - 1\n",
        "    activity_names = ['Walking', 'Walking Upstairs', 'Walking Downstairs', 'Sitting', 'Standing', 'Laying']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "    X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
        "    X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
        "\n",
        "    return X_train_scaled, y_train, X_test_scaled, y_test, activity_names\n",
        "\n",
        "class ImprovedTemporalMasking:\n",
        "    def __init__(self, mask_ratio=0.15, mask_length_range=(5, 15), noise_std=0.1):\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_length_range = mask_length_range\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        max_len = min(self.mask_length_range[1], T)\n",
        "        min_len = min(self.mask_length_range[0], T)\n",
        "\n",
        "        for b in range(B):\n",
        "            num_to_mask = int(T * self.mask_ratio)\n",
        "            while num_to_mask > 0:\n",
        "                length = np.random.randint(min_len, max_len + 1)\n",
        "                length = min(length, num_to_mask, T)\n",
        "                start = np.random.randint(0, T - length + 1)\n",
        "                mask[b, start:start + length] = False\n",
        "                num_to_mask -= length\n",
        "\n",
        "        masked_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        masked_x[:, :, ~mask[0]] = noise[:, :, ~mask[0]]\n",
        "\n",
        "        return masked_x, mask\n",
        "\n",
        "class ImprovedRandomPointDrop:\n",
        "    def __init__(self, drop_ratio=0.15, noise_std=0.1):\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        mask = torch.rand(B, T, device=x.device) > self.drop_ratio\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        dropped_x[:, :, ~mask[0]] = noise[:, :, ~mask[0]]\n",
        "\n",
        "        return dropped_x, mask\n",
        "\n",
        "class ImprovedChannelDrop:\n",
        "    def __init__(self, drop_prob=0.2, mode='random', noise_std=0.1):\n",
        "        self.drop_prob = drop_prob\n",
        "        self.mode = mode\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "\n",
        "        if self.mode == 'random':\n",
        "            channel_mask = torch.rand(B, C, device=x.device) > self.drop_prob\n",
        "        elif self.mode == 'axis':\n",
        "            axis_groups = [[0,3,6], [1,4,7], [2,5,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in axis_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "        elif self.mode == 'sensor':\n",
        "            sensor_groups = [[0,1,2], [3,4,5], [6,7,8]]\n",
        "            channel_mask = torch.ones(B, C, dtype=torch.bool, device=x.device)\n",
        "            for b in range(B):\n",
        "                for group in sensor_groups:\n",
        "                    if np.random.rand() < self.drop_prob:\n",
        "                        channel_mask[b, group] = False\n",
        "\n",
        "        dropped_x = x.clone()\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        for b in range(B):\n",
        "            for c in range(C):\n",
        "                if not channel_mask[b, c]:\n",
        "                    dropped_x[b, c, :] = noise[b, c, :]\n",
        "\n",
        "        temporal_mask = channel_mask.any(dim=1, keepdim=True).expand(B, T)\n",
        "        return dropped_x, temporal_mask\n",
        "\n",
        "class StandardAugmentation:\n",
        "    def __init__(self, noise_std=0.02, scale_range=(0.9, 1.1)):\n",
        "        self.noise_std = noise_std\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, T = x.shape\n",
        "        noise = torch.randn_like(x) * self.noise_std\n",
        "        augmented_x = x + noise\n",
        "        scale = torch.empty(B, C, 1, device=x.device).uniform_(*self.scale_range)\n",
        "        augmented_x = augmented_x * scale\n",
        "        mask = torch.ones(B, T, dtype=torch.bool, device=x.device)\n",
        "        return augmented_x, mask\n",
        "\n",
        "class ELKBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=31, deploy=False):\n",
        "        super().__init__()\n",
        "        self.deploy = deploy\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        padding_large1 = kernel_size // 2\n",
        "        kernel_size_large2 = kernel_size - 2\n",
        "        padding_large2 = kernel_size_large2 // 2\n",
        "        kernel_size_small1 = 5\n",
        "        padding_small1 = kernel_size_small1 // 2\n",
        "        kernel_size_small2 = 3\n",
        "        padding_small2 = kernel_size_small2 // 2\n",
        "\n",
        "        if deploy:\n",
        "            self.reparam_conv = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=True\n",
        "            )\n",
        "        else:\n",
        "            self.dw_large1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size,\n",
        "                padding=padding_large1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_large2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_large2,\n",
        "                padding=padding_large2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_large2 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small1 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small1,\n",
        "                padding=padding_small1, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small1 = nn.BatchNorm1d(in_channels)\n",
        "            self.dw_small2 = nn.Conv1d(\n",
        "                in_channels, in_channels, kernel_size_small2,\n",
        "                padding=padding_small2, groups=in_channels, bias=False\n",
        "            )\n",
        "            self.bn_small2 = nn.BatchNorm1d(in_channels)\n",
        "            self.bn_id = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "        self.pointwise = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.deploy:\n",
        "            x = self.reparam_conv(x)\n",
        "        else:\n",
        "            x1 = self.bn_large1(self.dw_large1(x))\n",
        "            x2 = self.bn_large2(self.dw_large2(x))\n",
        "            x3 = self.bn_small1(self.dw_small1(x))\n",
        "            x4 = self.bn_small2(self.dw_small2(x))\n",
        "            x5 = self.bn_id(x)\n",
        "            x = x1 + x2 + x3 + x4 + x5\n",
        "        x = self.activation(x)\n",
        "        return self.pointwise(x)\n",
        "\n",
        "class LightweightELKBackbone(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=1, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, d_model, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.elk_block = ELKBlock(d_model, d_model, kernel_size=kernel_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out_channels = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.elk_block(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "def improved_masked_pooling(h, mask, pool_type='attention'):\n",
        "    if pool_type == 'attention':\n",
        "        B, C, T = h.shape\n",
        "        mask_float = mask.float().unsqueeze(1)\n",
        "\n",
        "        attention_weights = torch.softmax(h.mean(dim=1, keepdim=True), dim=-1)\n",
        "        attention_weights = attention_weights * mask_float\n",
        "        attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        pooled = (h * attention_weights).sum(dim=-1)\n",
        "        return pooled\n",
        "\n",
        "    elif pool_type == 'weighted_avg':\n",
        "        mask_float = mask.unsqueeze(1).float()\n",
        "        numerator = (h * mask_float).sum(dim=-1)\n",
        "        denominator = mask_float.sum(dim=-1).clamp_min(1e-6)\n",
        "        return numerator / denominator\n",
        "\n",
        "    else:\n",
        "        return h.mean(dim=-1)\n",
        "\n",
        "class ImprovedELKEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=9, d_model=128, num_layers=1,\n",
        "                 kernel_size=31, output_dim=256, dropout=0.1, pool_type='attention'):\n",
        "        super().__init__()\n",
        "        self.backbone = LightweightELKBackbone(in_channels, d_model, num_layers, kernel_size, dropout)\n",
        "        self.pool_type = pool_type\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(d_model, output_dim),\n",
        "            nn.LayerNorm(output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        h = self.backbone(x)\n",
        "        if mask is not None:\n",
        "            h = improved_masked_pooling(h, mask, self.pool_type)\n",
        "        else:\n",
        "            h = h.mean(dim=-1)\n",
        "        z = self.projection(h)\n",
        "        z = F.normalize(z, dim=1)\n",
        "        return z\n",
        "\n",
        "class ImprovedMTCLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1, use_cosine=True):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.use_cosine = use_cosine\n",
        "\n",
        "    def forward(self, z_clean, z_masked):\n",
        "        if self.use_cosine:\n",
        "            sim = F.cosine_similarity(z_clean, z_masked, dim=1)\n",
        "            loss = -torch.log(torch.sigmoid(sim / self.temperature)).mean()\n",
        "        else:\n",
        "            loss = F.mse_loss(z_clean, z_masked)\n",
        "        return loss\n",
        "\n",
        "class ImprovedNTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07, use_hard_negatives=True):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.use_hard_negatives = use_hard_negatives\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        device = z1.device\n",
        "\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        sim_11 = (z1 @ z1.T) / self.temperature\n",
        "        sim_22 = (z2 @ z2.T) / self.temperature\n",
        "        sim_12 = (z1 @ z2.T) / self.temperature\n",
        "        sim_21 = sim_12.T\n",
        "\n",
        "        mask = torch.eye(B, device=device, dtype=torch.bool)\n",
        "        sim_11 = sim_11.masked_fill(mask, -9e15)\n",
        "        sim_22 = sim_22.masked_fill(mask, -9e15)\n",
        "\n",
        "        if self.use_hard_negatives:\n",
        "            hard_neg_weight = 2.0\n",
        "            sim_11 = sim_11 * hard_neg_weight\n",
        "            sim_22 = sim_22 * hard_neg_weight\n",
        "\n",
        "        logits_1 = torch.cat([sim_12, sim_11], dim=1)\n",
        "        logits_2 = torch.cat([sim_21, sim_22], dim=1)\n",
        "        labels = torch.arange(B, device=device)\n",
        "\n",
        "        loss_1 = F.cross_entropy(logits_1, labels)\n",
        "        loss_2 = F.cross_entropy(logits_2, labels)\n",
        "\n",
        "        return 0.5 * (loss_1 + loss_2)\n",
        "\n",
        "class ImprovedSSLFramework(nn.Module):\n",
        "    def __init__(self, method='mtc_micl', in_channels=9, d_model=128,\n",
        "                 num_layers=1, kernel_size=31, output_dim=256, dropout=0.1,\n",
        "                 mask_ratio=0.15, temperature=0.07,\n",
        "                 lambda_mtc=1.0, lambda_micl=1.0, channel_drop_mode='random',\n",
        "                 pool_type='attention'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.method = method\n",
        "        self.encoder = ImprovedELKEncoder(in_channels, d_model, num_layers,\n",
        "                                        kernel_size, output_dim, dropout, pool_type)\n",
        "\n",
        "        if method == 'sl_only':\n",
        "            self.augmentation = None\n",
        "        elif method == 'ssl_wo_missing':\n",
        "            self.augmentation = StandardAugmentation()\n",
        "        elif method == 'random_point_drop':\n",
        "            self.augmentation = ImprovedRandomPointDrop(drop_ratio=mask_ratio)\n",
        "        elif method == 'channel_drop':\n",
        "            self.augmentation = ImprovedChannelDrop(drop_prob=mask_ratio, mode=channel_drop_mode)\n",
        "        elif method == 'mtc_micl':\n",
        "            self.augmentation = ImprovedTemporalMasking(mask_ratio=mask_ratio)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "        self.mtc_loss = ImprovedMTCLoss(temperature=0.1)\n",
        "        self.micl_loss = ImprovedNTXentLoss(temperature=temperature)\n",
        "        self.lambda_mtc = lambda_mtc\n",
        "        self.lambda_micl = lambda_micl\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.method == 'sl_only':\n",
        "            return torch.tensor(0.0, device=x.device), {'total': 0.0, 'mtc': 0.0, 'micl': 0.0}\n",
        "\n",
        "        z_clean = self.encoder(x, mask=None)\n",
        "        x_aug, mask = self.augmentation(x)\n",
        "        z_aug = self.encoder(x_aug, mask=mask)\n",
        "\n",
        "        if self.method == 'ssl_wo_missing':\n",
        "            loss_mtc = torch.tensor(0.0, device=x.device)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = loss_micl\n",
        "        else:\n",
        "            loss_mtc = self.mtc_loss(z_clean, z_aug)\n",
        "            loss_micl = self.micl_loss(z_clean, z_aug)\n",
        "            loss = self.lambda_mtc * loss_mtc + self.lambda_micl * loss_micl\n",
        "\n",
        "        losses_dict = {\n",
        "            'total': loss.item(),\n",
        "            'mtc': loss_mtc.item(),\n",
        "            'micl': loss_micl.item()\n",
        "        }\n",
        "\n",
        "        return loss, losses_dict\n",
        "\n",
        "    def get_representation(self, x, mask=None):\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(x, mask=mask)\n",
        "\n",
        "class TwoStageLinearClassifier(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        self._freeze_encoder()\n",
        "\n",
        "    def _freeze_encoder(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def _unfreeze_encoder(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x, mask=None)\n",
        "        return self.classifier(z)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def calculate_flops(model, input_shape=(1, 9, 128), device='cuda'):\n",
        "    model.eval()\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "\n",
        "    def conv_flop_count(input_shape, weight_shape, stride=1, padding=0):\n",
        "        batch_size, in_channels, input_length = input_shape\n",
        "        out_channels, in_channels, kernel_size = weight_shape\n",
        "        output_length = (input_length + 2 * padding - kernel_size) // stride + 1\n",
        "        return batch_size * out_channels * output_length * in_channels * kernel_size\n",
        "\n",
        "    def linear_flop_count(input_features, output_features):\n",
        "        return input_features * output_features\n",
        "\n",
        "    total_flops = 0\n",
        "\n",
        "    def flop_hook(module, input, output):\n",
        "        nonlocal total_flops\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            input_shape = input[0].shape\n",
        "            weight_shape = module.weight.shape\n",
        "            stride = module.stride[0] if isinstance(module.stride, tuple) else module.stride\n",
        "            padding = module.padding[0] if isinstance(module.padding, tuple) else module.padding\n",
        "            flops = conv_flop_count(input_shape, weight_shape, stride, padding)\n",
        "            total_flops += flops\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            input_features = input[0].shape[-1]\n",
        "            output_features = module.out_features\n",
        "            batch_size = input[0].shape[0]\n",
        "            flops = batch_size * linear_flop_count(input_features, output_features)\n",
        "            total_flops += flops\n",
        "\n",
        "    hooks = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
        "            hooks.append(module.register_forward_hook(flop_hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
        "    model.eval()\n",
        "    total_time = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            x = x.to(device)\n",
        "\n",
        "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
        "            start_time = time.time()\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
        "            end_time = time.time()\n",
        "\n",
        "            total_time += end_time - start_time\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    return total_time / total_samples * 1000\n",
        "\n",
        "def train_ssl_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mtc = 0\n",
        "    total_micl = 0\n",
        "\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, losses_dict = model(x)\n",
        "        if loss.item() > 0:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "        total_loss += losses_dict['total']\n",
        "        total_mtc += losses_dict['mtc']\n",
        "        total_micl += losses_dict['micl']\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'mtc': total_mtc / num_batches,\n",
        "        'micl': total_micl / num_batches\n",
        "    }\n",
        "\n",
        "def train_two_stage_linear(model, train_loader, test_loader, device,\n",
        "                          stage1_epochs=75, stage2_epochs=25,\n",
        "                          stage1_lr=1e-3, stage2_lr=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_test_acc = 0.0\n",
        "\n",
        "    print(f\"  Stage 1: Frozen encoder + classifier training ({stage1_epochs} epochs)\")\n",
        "    model._freeze_encoder()\n",
        "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=stage1_lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage1_epochs)\n",
        "\n",
        "    for epoch in range(stage1_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "        test_acc = evaluate_linear(model, test_loader, device)\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 15 == 0:\n",
        "            print(f\"    Epoch {epoch+1}/{stage1_epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "    print(f\"  Stage 2: Fine-tuning entire model ({stage2_epochs} epochs)\")\n",
        "    model._unfreeze_encoder()\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': model.encoder.parameters(), 'lr': stage2_lr},\n",
        "        {'params': model.classifier.parameters(), 'lr': stage1_lr}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_groups, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stage2_epochs)\n",
        "\n",
        "    for epoch in range(stage2_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "        test_acc = evaluate_linear(model, test_loader, device)\n",
        "        best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"    Epoch {epoch+1}/{stage2_epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "\n",
        "    return best_test_acc\n",
        "\n",
        "def evaluate_linear(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "def evaluate_missing_robustness_improved(model, dataloader, device, method_name, missing_ratios=[0.0, 0.15, 0.3, 0.5]):\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for ratio in missing_ratios:\n",
        "        if method_name == 'ssl_wo_missing' and ratio > 0:\n",
        "            results[ratio] = 0.0\n",
        "            continue\n",
        "\n",
        "        if method_name == 'random_point_drop':\n",
        "            masking = ImprovedRandomPointDrop(drop_ratio=ratio)\n",
        "        elif 'channel_drop' in method_name:\n",
        "            mode = 'sensor' if 'sensor' in method_name else 'random'\n",
        "            masking = ImprovedChannelDrop(drop_prob=ratio, mode=mode)\n",
        "        else:\n",
        "            masking = ImprovedTemporalMasking(mask_ratio=ratio)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in dataloader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                if ratio > 0:\n",
        "                    x_masked, mask = masking(x)\n",
        "                    logits = model(x_masked)\n",
        "                else:\n",
        "                    logits = model(x)\n",
        "\n",
        "                pred = logits.argmax(dim=1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        results[ratio] = 100.0 * correct / total\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_improved_benchmark(data_dir, device, ssl_epochs=150, batch_size=128):\n",
        "    print(\"=\"*80)\n",
        "    print(\"IMPROVED BENCHMARK: Lightweight ELK with Two-Stage Training\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_train, y_train, X_test, y_test, activity_names = load_uci_har_raw(data_dir)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    methods = [\n",
        "        {'name': 'SL-Only', 'method': 'sl_only', 'desc': 'Supervised learning only'},\n",
        "        {'name': 'SSL w/o Missing', 'method': 'ssl_wo_missing', 'desc': 'Standard contrastive'},\n",
        "        {'name': 'Random Point Drop', 'method': 'random_point_drop', 'desc': 'Non-contiguous missing'},\n",
        "        {'name': 'Channel Drop (Random)', 'method': 'channel_drop', 'desc': 'Random channel missing', 'channel_mode': 'random'},\n",
        "        {'name': 'Channel Drop (Sensor)', 'method': 'channel_drop', 'desc': 'Sensor-wise missing', 'channel_mode': 'sensor'},\n",
        "        {'name': 'MTC + MICL (Ours)', 'method': 'mtc_micl', 'desc': 'Improved temporal masking'},\n",
        "    ]\n",
        "\n",
        "    results = defaultdict(dict)\n",
        "\n",
        "    for config in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Method: {config['name']}\")\n",
        "        print(f\"Description: {config['desc']}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        if config['method'] != 'sl_only':\n",
        "            print(f\"\\n[Phase 1] SSL Pretraining ({ssl_epochs} epochs)...\")\n",
        "\n",
        "            ssl_model = ImprovedSSLFramework(\n",
        "                method=config['method'],\n",
        "                channel_drop_mode=config.get('channel_mode', 'random'),\n",
        "                mask_ratio=0.15,\n",
        "                num_layers=1\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ssl_epochs)\n",
        "\n",
        "            for epoch in range(ssl_epochs):\n",
        "                metrics = train_ssl_epoch(ssl_model, train_loader, optimizer, device)\n",
        "                scheduler.step()\n",
        "\n",
        "                if (epoch + 1) % 25 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/{ssl_epochs}: Loss={metrics['loss']:.4f}, \"\n",
        "                          f\"MTC={metrics['mtc']:.4f}, MICL={metrics['micl']:.4f}\")\n",
        "\n",
        "            encoder = ssl_model.encoder\n",
        "        else:\n",
        "            encoder = ImprovedELKEncoder(num_layers=1).to(device)\n",
        "\n",
        "        if config['method'] == 'sl_only':\n",
        "            print(f\"\\n[Phase 2] Supervised Learning (100 epochs)...\")\n",
        "            linear_model = TwoStageLinearClassifier(encoder, num_classes=6).to(device)\n",
        "            linear_model._unfreeze_encoder()\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.AdamW(linear_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "            best_test_acc = 0.0\n",
        "            for epoch in range(100):\n",
        "                linear_model.train()\n",
        "                total_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for x, y in train_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = linear_model(x)\n",
        "                    loss = criterion(logits, y)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(linear_model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    pred = logits.argmax(dim=1)\n",
        "                    correct += (pred == y).sum().item()\n",
        "                    total += y.size(0)\n",
        "\n",
        "                scheduler.step()\n",
        "                train_acc = 100.0 * correct / total\n",
        "                test_acc = evaluate_linear(linear_model, test_loader, device)\n",
        "                best_test_acc = max(best_test_acc, test_acc)\n",
        "\n",
        "                if (epoch + 1) % 20 == 0:\n",
        "                    print(f\"  Epoch {epoch+1}/100: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "        else:\n",
        "            print(f\"\\n[Phase 2] Two-Stage Linear Evaluation...\")\n",
        "            linear_model = TwoStageLinearClassifier(encoder, num_classes=6).to(device)\n",
        "\n",
        "            best_test_acc = train_two_stage_linear(\n",
        "                linear_model, train_loader, test_loader, device,\n",
        "                stage1_epochs=75, stage2_epochs=25,\n",
        "                stage1_lr=1e-3, stage2_lr=1e-5\n",
        "            )\n",
        "\n",
        "        results[config['name']]['test_accuracy'] = best_test_acc\n",
        "\n",
        "        total_params = count_parameters(linear_model)\n",
        "        encoder_params = count_parameters(linear_model.encoder)\n",
        "        total_flops = calculate_flops(linear_model, input_shape=(1, 9, 128), device=device)\n",
        "        inference_time = measure_inference_time(linear_model, test_loader, device)\n",
        "\n",
        "        results[config['name']]['total_params'] = total_params\n",
        "        results[config['name']]['encoder_params'] = encoder_params\n",
        "        results[config['name']]['total_flops'] = total_flops\n",
        "        results[config['name']]['inference_time_ms'] = inference_time\n",
        "\n",
        "        print(f\"\\n[Phase 3] Missing Robustness Test...\")\n",
        "        missing_results = evaluate_missing_robustness_improved(\n",
        "            linear_model, test_loader, device, config['name'],\n",
        "            missing_ratios=[0.0, 0.15, 0.3, 0.5]\n",
        "        )\n",
        "        results[config['name']]['missing_robustness'] = missing_results\n",
        "\n",
        "        for ratio, acc in missing_results.items():\n",
        "            print(f\"  Missing Ratio {ratio:.2f}: {acc:.2f}%\")\n",
        "\n",
        "        print(f\"\\n[Model Statistics]\")\n",
        "        print(f\"  Total Parameters: {total_params:,}\")\n",
        "        print(f\"  Encoder Parameters: {encoder_params:,}\")\n",
        "        print(f\"  Total FLOPs: {total_flops/1e6:.2f}M\")\n",
        "        print(f\"  Inference Time: {inference_time:.2f} ms/sample\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"IMPROVED BENCHMARK SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Method':<25} {'Clean Acc':<10} {'15% Miss':<10} {'30% Miss':<10} {'50% Miss':<10} {'Params':<8} {'FLOPs':<8} {'Time(ms)':<8}\")\n",
        "    print(\"-\"*105)\n",
        "\n",
        "    for method_name, result in results.items():\n",
        "        clean_acc = result['missing_robustness'][0.0]\n",
        "        miss_15 = result['missing_robustness'].get(0.15, 0.0)\n",
        "        miss_30 = result['missing_robustness'].get(0.3, 0.0)\n",
        "        miss_50 = result['missing_robustness'].get(0.5, 0.0)\n",
        "        params = result['total_params'] // 1000\n",
        "        flops = result['total_flops'] / 1e6\n",
        "        inf_time = result['inference_time_ms']\n",
        "\n",
        "        print(f\"{method_name:<25} {clean_acc:>8.1f}% {miss_15:>8.1f}% \"\n",
        "              f\"{miss_30:>8.1f}% {miss_50:>8.1f}% {params:>6}K {flops:>6.1f}M {inf_time:>6.1f}\")\n",
        "\n",
        "    with open('improved_benchmark_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\n✓ Results saved to: improved_benchmark_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    data_dir = '/content/'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\\n\")\n",
        "\n",
        "    results = run_improved_benchmark(\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        ssl_epochs=150,\n",
        "        batch_size=128\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJkwSDjsMYgs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}